{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolutional Autoencoder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/home/hao/miniconda3/envs/tensorflow/lib/python3.5/site-packages/h5py/__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf \n",
    "import matplotlib.pyplot as plt\n",
    "import os \n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.image import imread\n",
    "from sklearn.utils import shuffle\n",
    "from scipy.misc import imsave\n",
    "from scipy.misc import imresize\n",
    "import numpy as np\n",
    "from numpy import genfromtxt\n",
    "import tensorflow as tf\n",
    "from PIL import Image\n",
    "import pickle\n",
    "# from tensorflow_vgg import vgg16|\n",
    "import collections\n",
    "import time\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Checkpoint 1\n",
    "# Load Parameters of individual ITCNets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from /usr/home/hao/work_space/FP/itcnets/checkpoints_view_invariant/choosed/1to3_add-more-data_lr=6e-05_nl=3_bs=80_dim=128.ckpt\n"
     ]
    }
   ],
   "source": [
    "# 1f - 3f\n",
    "loaded_graph = tf.Graph()\n",
    "save_model_path = '/usr/home/hao/work_space/FP/itcnets/checkpoints_view_invariant/choosed/1to3_add-more-data_lr=6e-05_nl=3_bs=80_dim=128.ckpt'\n",
    "with tf.Session(graph=loaded_graph) as sess:\n",
    "    # Load model\n",
    "    loader = tf.train.import_meta_graph(save_model_path + '.meta')\n",
    "    loader.restore(sess, save_model_path)\n",
    "\n",
    "    # Get Tensors from loaded model\n",
    "    \n",
    "    Au1w1 = loaded_graph.get_tensor_by_name('conv_layers/conv_weights_1:0').eval()\n",
    "    Au1b1 = loaded_graph.get_tensor_by_name('conv_layers/bias_1:0').eval()\n",
    "    Au1w2 = loaded_graph.get_tensor_by_name('conv_layers/conv_weights_2:0').eval()\n",
    "    Au1b2 = loaded_graph.get_tensor_by_name('conv_layers/bias_2:0').eval()\n",
    "    Au1w3 = loaded_graph.get_tensor_by_name('conv_layers/conv_weights_3:0').eval()\n",
    "    Au1b3 = loaded_graph.get_tensor_by_name('conv_layers/bias_3:0').eval()\n",
    "    Au1deW1 = loaded_graph.get_tensor_by_name('deconv_layers/deconv_weights_1:0').eval()\n",
    "    Au1deB1 = loaded_graph.get_tensor_by_name('deconv_layers/debias_1:0').eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from /usr/home/hao/work_space/FP/itcnets/checkpoints_view_invariant/choosed/3to5_add-more-data_lr=6e-05_nl=3_bs=80_dim=128.ckpt\n"
     ]
    }
   ],
   "source": [
    "# 3f - 5f\n",
    "loaded_graph = tf.Graph()\n",
    "save_model_path = '/usr/home/hao/work_space/FP/itcnets/checkpoints_view_invariant/choosed/3to5_add-more-data_lr=6e-05_nl=3_bs=80_dim=128.ckpt'\n",
    "with tf.Session(graph=loaded_graph) as sess:\n",
    "    # Load model\n",
    "    loader = tf.train.import_meta_graph(save_model_path + '.meta')\n",
    "    loader.restore(sess, save_model_path)\n",
    "\n",
    "    # Get Tensors from loaded model\n",
    "    \n",
    "    Au2deW1 = loaded_graph.get_tensor_by_name('deconv_layers/deconv_weights_1:0').eval()\n",
    "    Au2deB1 = loaded_graph.get_tensor_by_name('deconv_layers/debias_1:0').eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from /usr/home/hao/work_space/FP/itcnets/checkpoints_view_invariant/choosed/5to8_add-more-data_lr=6e-05_nl=3_bs=80_dim=128.ckpt\n"
     ]
    }
   ],
   "source": [
    "# 5f - 8f\n",
    "loaded_graph = tf.Graph()\n",
    "save_model_path = '/usr/home/hao/work_space/FP/itcnets/checkpoints_view_invariant/choosed/5to8_add-more-data_lr=6e-05_nl=3_bs=80_dim=128.ckpt'\n",
    "with tf.Session(graph=loaded_graph) as sess:\n",
    "    # Load model\n",
    "    loader = tf.train.import_meta_graph(save_model_path + '.meta')\n",
    "    loader.restore(sess, save_model_path)\n",
    "\n",
    "    # Get Tensors from loaded model\n",
    "    \n",
    "    Au3deW1 = loaded_graph.get_tensor_by_name('deconv_layers/deconv_weights_1:0').eval()\n",
    "    Au3deB1 = loaded_graph.get_tensor_by_name('deconv_layers/debias_1:0').eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from /usr/home/hao/work_space/FP/itcnets/checkpoints_view_invariant/choosed/8to10_add-more-data_lr=6e-05_nl=3_bs=80_dim=128.ckpt\n"
     ]
    }
   ],
   "source": [
    "# 8f - 10f\n",
    "loaded_graph = tf.Graph()\n",
    "save_model_path = '/usr/home/hao/work_space/FP/itcnets/checkpoints_view_invariant/choosed/8to10_add-more-data_lr=6e-05_nl=3_bs=80_dim=128.ckpt'\n",
    "with tf.Session(graph=loaded_graph) as sess:\n",
    "    # Load model\n",
    "    loader = tf.train.import_meta_graph(save_model_path + '.meta')\n",
    "    loader.restore(sess, save_model_path)\n",
    "\n",
    "    # Get Tensors from loaded model\n",
    "    Au4deW1 = loaded_graph.get_tensor_by_name('deconv_layers/deconv_weights_1:0').eval()\n",
    "    Au4deB1 = loaded_graph.get_tensor_by_name('deconv_layers/debias_1:0').eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from /usr/home/hao/work_space/FP/itcnets/checkpoints_view_invariant/choosed/10to13_add-more-data_lr=6e-05_nl=3_bs=80_dim=128.ckpt\n"
     ]
    }
   ],
   "source": [
    "# 10f - 13f\n",
    "loaded_graph = tf.Graph()\n",
    "save_model_path = '/usr/home/hao/work_space/FP/itcnets/checkpoints_view_invariant/choosed/10to13_add-more-data_lr=6e-05_nl=3_bs=80_dim=128.ckpt'\n",
    "with tf.Session(graph=loaded_graph) as sess:\n",
    "    # Load model\n",
    "    loader = tf.train.import_meta_graph(save_model_path + '.meta')\n",
    "    loader.restore(sess, save_model_path)\n",
    "\n",
    "    # Get Tensors from loaded model\n",
    "    Au5deW1 = loaded_graph.get_tensor_by_name('deconv_layers/deconv_weights_1:0').eval()\n",
    "    Au5deB1 = loaded_graph.get_tensor_by_name('deconv_layers/debias_1:0').eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from /usr/home/hao/work_space/FP/itcnets/checkpoints_view_invariant/choosed/13to15_add-more-data_lr=6e-05_nl=3_bs=80_dim=128.ckpt\n"
     ]
    }
   ],
   "source": [
    "# 13f - 15f\n",
    "loaded_graph = tf.Graph()\n",
    "save_model_path = '/usr/home/hao/work_space/FP/itcnets/checkpoints_view_invariant/choosed/13to15_add-more-data_lr=6e-05_nl=3_bs=80_dim=128.ckpt'\n",
    "with tf.Session(graph=loaded_graph) as sess:\n",
    "    # Load model\n",
    "    loader = tf.train.import_meta_graph(save_model_path + '.meta')\n",
    "    loader.restore(sess, save_model_path)\n",
    "\n",
    "    # Get Tensors from loaded model\n",
    "    Au6deW1 = loaded_graph.get_tensor_by_name('deconv_layers/deconv_weights_1:0').eval()\n",
    "    Au6deB1 = loaded_graph.get_tensor_by_name('deconv_layers/debias_1:0').eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from /usr/home/hao/work_space/FP/itcnets/checkpoints_view_invariant/choosed/15to18_add-more-data_lr=6e-05_nl=3_bs=80_dim=128.ckpt\n"
     ]
    }
   ],
   "source": [
    "# 15f - 18f\n",
    "loaded_graph = tf.Graph()\n",
    "save_model_path = '/usr/home/hao/work_space/FP/itcnets/checkpoints_view_invariant/choosed/15to18_add-more-data_lr=6e-05_nl=3_bs=80_dim=128.ckpt'\n",
    "with tf.Session(graph=loaded_graph) as sess:\n",
    "    # Load model\n",
    "    loader = tf.train.import_meta_graph(save_model_path + '.meta')\n",
    "    loader.restore(sess, save_model_path)\n",
    "\n",
    "    # Get Tensors from loaded model\n",
    "    Au7deW1 = loaded_graph.get_tensor_by_name('deconv_layers/deconv_weights_1:0').eval()\n",
    "    Au7deB1 = loaded_graph.get_tensor_by_name('deconv_layers/debias_1:0').eval()\n",
    "    # Val X1\n",
    "# val_x = gallery_full[2500:, 0]\n",
    "# val_x = np.reshape(val_x, [val_x.shape[0], val_x.shape[1], val_x.shape[2], 1])\n",
    "# # Val Y\n",
    "# val_y = gallery_full[2500:, 3]\n",
    "# val_y = np.reshape(val_y, [val_y.shape[0], val_y.shape[1], val_y.shape[2], 1])\n",
    "# print('val_x', val_x.shape, 'valsave_decoded_y', val_y.shape)\n",
    "# me('deconv_layers/debias_1:0').eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from /usr/home/hao/work_space/FP/itcnets/checkpoints_view_invariant/choosed/18to20_add-more-data_lr=6e-05_nl=3_bs=80_dim=128.ckpt\n"
     ]
    }
   ],
   "source": [
    "# 18f - 20f\n",
    "loaded_graph = tf.Graph()\n",
    "save_model_path = '/usr/home/hao/work_space/FP/itcnets/checkpoints_view_invariant/choosed/18to20_add-more-data_lr=6e-05_nl=3_bs=80_dim=128.ckpt'\n",
    "with tf.Session(graph=loaded_graph) as sess:\n",
    "    # Load model\n",
    "    loader = tf.train.import_meta_graph(save_model_path + '.meta')\n",
    "    loader.restore(sess, save_model_path)\n",
    "\n",
    "    # Get Tensors from loaded model\n",
    "    Au8deW1 = loaded_graph.get_tensor_by_name('deconv_layers/deconv_weights_1:0').eval()\n",
    "    Au8deB1 = loaded_graph.get_tensor_by_name('deconv_layers/debias_1:0').eval()\n",
    "#     Au3deW2 = loaded_graph.get_tensor_by_name('deconv_layers/deconv_weights_2:0').eval()\n",
    "#     Au3deB2 = loaded_graph.get_tensor_by_name('deconv_layers/debias_2:0').eval()\n",
    "#     Au3outW = loaded_graph.get_tensor_by_name('output_layer/output_weights:0').eval()\n",
    "#     Au3outb = loaded_graph.get_tensor_by_name('output_layer/outbias_1:0').eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from /usr/home/hao/work_space/FP/itcnets/checkpoints_view_invariant/choosed/20tofull_add-more-data_lr=6e-05_nl=3_bs=80_dim=128.ckpt\n"
     ]
    }
   ],
   "source": [
    "# 20f - full\n",
    "loaded_graph = tf.Graph()\n",
    "save_model_path = '/usr/home/hao/work_space/FP/itcnets/checkpoints_view_invariant/choosed/20tofull_add-more-data_lr=6e-05_nl=3_bs=80_dim=128.ckpt'\n",
    "with tf.Session(graph=loaded_graph) as sess:\n",
    "    # Load model\n",
    "    loader = tf.train.import_meta_graph(save_model_path + '.meta')\n",
    "    loader.restore(sess, save_model_path)\n",
    "\n",
    "    # Get Tensors from loaded model\n",
    "    Au9deW1 = loaded_graph.get_tensor_by_name('deconv_layers/deconv_weights_1:0').eval()\n",
    "    Au9deB1 = loaded_graph.get_tensor_by_name('deconv_layers/debias_1:0').eval()\n",
    "    Au9deW2 = loaded_graph.get_tensor_by_name('deconv_layers/deconv_weights_2:0').eval()\n",
    "    Au9deB2 = loaded_graph.get_tensor_by_name('deconv_layers/debias_2:0').eval()\n",
    "    Au9deW3 = loaded_graph.get_tensor_by_name('deconv_layers/deconv_weights_3:0').eval()\n",
    "    Au9deB3 = loaded_graph.get_tensor_by_name('deconv_layers/debias_3:0').eval()\n",
    "    Au9outW = loaded_graph.get_tensor_by_name('output_layer/output_weights:0').eval()\n",
    "    Au9outB = loaded_graph.get_tensor_by_name('output_layer/outbias_1:0').eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br><br><br><br><br><br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data\n",
    "here you can load any data you want, we use OULP_GEI' path as an example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Training Data\n",
    "# Subjects_list = np.load(open(r'../gait_data/OULP_GEI/GEI_subjects_list', mode='rb'))\n",
    "\n",
    "# These are pre-generated pathes of different GEIs, you can generate yours.\n",
    "GEI_1f_path = np.load(open(r'/usr/home/hao/work_space/FP/ITCNets/gait-data/OULP_GEI/GEI_all_1f_path', mode='rb'))\n",
    "GEI_2f_path = np.load(open(r'/usr/home/hao/work_space/FP/ITCNets/gait-data/OULP_GEI/GEI_all_3f_path', mode='rb'))\n",
    "GEI_4f_path = np.load(open(r'/usr/home/hao/work_space/FP/ITCNets/gait-data/OULP_GEI/GEI_all_5f_path', mode='rb'))\n",
    "GEI_6f_path = np.load(open(r'/usr/home/hao/work_space/FP/ITCNets/gait-data/OULP_GEI/GEI_all_8f_path', mode='rb'))\n",
    "GEI_8f_path = np.load(open(r'/usr/home/hao/work_space/FP/ITCNets/gait-data/OULP_GEI/GEI_all_10f_path', mode='rb'))\n",
    "GEI_10f_path = np.load(open(r'/usr/home/hao/work_space/FP/ITCNets/gait-data/OULP_GEI/GEI_all_13f_path', mode='rb'))\n",
    "GEI_13f_path = np.load(open(r'/usr/home/hao/work_space/FP/ITCNets/gait-data/OULP_GEI/GEI_all_15f_path', mode='rb'))\n",
    "GEI_15f_path = np.load(open(r'/usr/home/hao/work_space/FP/ITCNets/gait-data/OULP_GEI/GEI_all_18f_path', mode='rb'))\n",
    "GEI_17f_path = np.load(open(r'/usr/home/hao/work_space/FP/ITCNets/gait-data/OULP_GEI/GEI_all_20f_path', mode='rb'))\n",
    "# test_full = np.load(open(r'../gait_data/casia_b_GEI/GEI_full', mode='rb'))\n",
    "GEI_full_path = np.load(open(r'/usr/home/hao/work_space/FP/ITCNets/gait-data/OULP_GEI/GEI_all_full_path', mode='rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['./OULP-C1V2_Pack/OULP-GEI-(88x128)-full/Seq00/3000588/3000588_85_GEI_30f.png',\n",
       "       './OULP-C1V2_Pack/OULP-GEI-(88x128)-full/Seq00/6170951/6170951_85_GEI_30f.png',\n",
       "       './OULP-C1V2_Pack/OULP-GEI-(88x128)-full/Seq00/3000460/3000460_85_GEI_30f.png',\n",
       "       ...,\n",
       "       './OULP-C1V2_Pack/OULP-GEI-(88x128)-full/Seq01/6159038/6159038_85_GEI_30f.png',\n",
       "       './OULP-C1V2_Pack/OULP-GEI-(88x128)-full/Seq01/6118547/6118547_85_GEI_30f.png',\n",
       "       './OULP-C1V2_Pack/OULP-GEI-(88x128)-full/Seq01/3000212/3000212_85_GEI_30f.png'],\n",
       "      dtype='<U76')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "GEI_full_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data split "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get train, validation, test data\n",
    "train_x = np.array([])\n",
    "train_y = np.array([])\n",
    "\n",
    "val_x = np.array([])\n",
    "val_y = np.array([])\n",
    "\n",
    "test_x = np.array([])\n",
    "test_y = np.array([])\n",
    "\n",
    "for ii in range(18):\n",
    "    train_x = np.concatenate([train_x, GEI_1f_path[ii*50:ii*50 + 40]], axis=0)\n",
    "    train_y = np.concatenate([train_y, GEI_full_path[ii*50:ii*50 + 40]], axis=0)\n",
    "    \n",
    "    train_x = np.concatenate([train_x, GEI_2f_path[ii*50:ii*50 + 40]], axis=0)\n",
    "    train_y = np.concatenate([train_y, GEI_full_path[ii*50:ii*50 + 40]], axis=0)\n",
    "    \n",
    "    train_x = np.concatenate([train_x, GEI_4f_path[ii*50:ii*50 + 40]], axis=0)\n",
    "    train_y = np.concatenate([train_y, GEI_full_path[ii*50:ii*50 + 40]], axis=0)\n",
    "    \n",
    "    train_x = np.concatenate([train_x, GEI_6f_path[ii*50:ii*50 + 40]], axis=0)\n",
    "    train_y = np.concatenate([train_y, GEI_full_path[ii*50:ii*50 + 40]], axis=0)\n",
    "    \n",
    "    train_x = np.concatenate([train_x, GEI_8f_path[ii*50:ii*50 + 40]], axis=0)\n",
    "    train_y = np.concatenate([train_y, GEI_full_path[ii*50:ii*50 + 40]], axis=0)\n",
    "    \n",
    "    train_x = np.concatenate([train_x, GEI_10f_path[ii*50:ii*50 + 40]], axis=0)\n",
    "    train_y = np.concatenate([train_y, GEI_full_path[ii*50:ii*50 + 40]], axis=0)\n",
    "    \n",
    "    train_x = np.concatenate([train_x, GEI_13f_path[ii*50:ii*50 + 40]], axis=0)\n",
    "    train_y = np.concatenate([train_y, GEI_full_path[ii*50:ii*50 + 40]], axis=0)\n",
    "    \n",
    "    train_x = np.concatenate([train_x, GEI_15f_path[ii*50:ii*50 + 40]], axis=0)\n",
    "    train_y = np.concatenate([train_y, GEI_full_path[ii*50:ii*50 + 40]], axis=0)\n",
    "    \n",
    "    train_x = np.concatenate([train_x, GEI_17f_path[ii*50:ii*50 + 40]], axis=0)\n",
    "    train_y = np.concatenate([train_y, GEI_full_path[ii*50:ii*50 + 40]], axis=0)\n",
    "    \n",
    "    \n",
    "    # val\n",
    "    val_x = np.concatenate([val_x, GEI_1f_path[ii*50 + 40:ii*50 + 46]], axis=0)\n",
    "    val_y = np.concatenate([val_y, GEI_full_path[ii*50 + 40:ii*50 + 46]], axis=0)\n",
    "    \n",
    "    val_x = np.concatenate([val_x, GEI_2f_path[ii*50 + 40:ii*50 + 46]], axis=0)\n",
    "    val_y = np.concatenate([val_y, GEI_full_path[ii*50 + 40:ii*50 + 46]], axis=0)\n",
    "    \n",
    "    val_x = np.concatenate([val_x, GEI_4f_path[ii*50 + 40:ii*50 + 46]], axis=0)\n",
    "    val_y = np.concatenate([val_y, GEI_full_path[ii*50 + 40:ii*50 + 46]], axis=0)\n",
    "    \n",
    "    val_x = np.concatenate([val_x, GEI_6f_path[ii*50 + 40:ii*50 + 46]], axis=0)\n",
    "    val_y = np.concatenate([val_y, GEI_full_path[ii*50 + 40:ii*50 + 46]], axis=0)\n",
    "    \n",
    "    val_x = np.concatenate([val_x, GEI_8f_path[ii*50 + 40:ii*50 + 46]], axis=0)\n",
    "    val_y = np.concatenate([val_y, GEI_full_path[ii*50 + 40:ii*50 + 46]], axis=0)\n",
    "    \n",
    "    val_x = np.concatenate([val_x, GEI_10f_path[ii*50 + 40:ii*50 + 46]], axis=0)\n",
    "    val_y = np.concatenate([val_y, GEI_full_path[ii*50 + 40:ii*50 + 46]], axis=0)\n",
    "    \n",
    "    val_x = np.concatenate([val_x, GEI_13f_path[ii*50 + 40:ii*50 + 46]], axis=0)\n",
    "    val_y = np.concatenate([val_y, GEI_full_path[ii*50 + 40:ii*50 + 46]], axis=0)\n",
    "    \n",
    "    val_x = np.concatenate([val_x, GEI_15f_path[ii*50 + 40:ii*50 + 46]], axis=0)\n",
    "    val_y = np.concatenate([val_y, GEI_full_path[ii*50 + 40:ii*50 + 46]], axis=0)\n",
    "    \n",
    "    val_x = np.concatenate([val_x, GEI_17f_path[ii*50 + 40:ii*50 + 46]], axis=0)\n",
    "    val_y = np.concatenate([val_y, GEI_full_path[ii*50 + 40:ii*50 + 46]], axis=0)\n",
    "    \n",
    "    \n",
    "    # test\n",
    "    test_x = np.concatenate([test_x, GEI_1f_path[ii*50 + 46:ii*50 + 50]], axis=0)\n",
    "    test_y = np.concatenate([test_y, GEI_full_path[ii*50 + 46:ii*50 + 50]], axis=0)\n",
    "    \n",
    "    test_x = np.concatenate([test_x, GEI_2f_path[ii*50 + 46:ii*50 + 50]], axis=0)\n",
    "    test_y = np.concatenate([test_y, GEI_full_path[ii*50 + 46:ii*50 + 50]], axis=0)\n",
    "    \n",
    "    test_x = np.concatenate([test_x, GEI_4f_path[ii*50 + 46:ii*50 + 50]], axis=0)\n",
    "    test_y = np.concatenate([test_y, GEI_full_path[ii*50 + 46:ii*50 + 50]], axis=0)\n",
    "    \n",
    "    test_x = np.concatenate([test_x, GEI_6f_path[ii*50 + 46:ii*50 + 50]], axis=0)\n",
    "    test_y = np.concatenate([test_y, GEI_full_path[ii*50 + 46:ii*50 + 50]], axis=0)\n",
    "    \n",
    "    test_x = np.concatenate([test_x, GEI_8f_path[ii*50 + 46:ii*50 + 50]], axis=0)\n",
    "    test_y = np.concatenate([test_y, GEI_full_path[ii*50 + 46:ii*50 + 50]], axis=0)\n",
    "    \n",
    "    test_x = np.concatenate([test_x, GEI_10f_path[ii*50 + 46:ii*50 + 50]], axis=0)\n",
    "    test_y = np.concatenate([test_y, GEI_full_path[ii*50 + 46:ii*50 + 50]], axis=0)\n",
    "    \n",
    "    test_x = np.concatenate([test_x, GEI_13f_path[ii*50 + 46:ii*50 + 50]], axis=0)\n",
    "    test_y = np.concatenate([test_y, GEI_full_path[ii*50 + 46:ii*50 + 50]], axis=0)\n",
    "    \n",
    "    test_x = np.concatenate([test_x, GEI_15f_path[ii*50 + 46:ii*50 + 50]], axis=0)\n",
    "    test_y = np.concatenate([test_y, GEI_full_path[ii*50 + 46:ii*50 + 50]], axis=0)\n",
    "    \n",
    "    test_x = np.concatenate([test_x, GEI_17f_path[ii*50 + 46:ii*50 + 50]], axis=0)\n",
    "    test_y = np.concatenate([test_y, GEI_full_path[ii*50 + 46:ii*50 + 50]], axis=0)\n",
    "    \n",
    "train_x, train_y = shuffle(train_x, train_y)\n",
    "val_x, val_y = shuffle(val_x, val_y)\n",
    "test_x, test_y = shuffle(test_x, test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "For_test_x = np.array([])\n",
    "For_test_y = np.array([])\n",
    "# data used for test, save them, next time direct read the For_test_x, and For_test_y\n",
    "for n_frames in [GEI_1f_path, GEI_2f_path, GEI_4f_path, GEI_6f_path, GEI_8f_path, GEI_10f_path, GEI_13f_path, GEI_15f_path, GEI_17f_path]:\n",
    "    \n",
    "    for ii in range(18):\n",
    "        For_test_x = np.concatenate([For_test_x, n_frames[40 + ii*50 : 46 + ii*50]], axis=0) \n",
    "        For_test_y = np.concatenate([For_test_y, GEI_full_path[40 + ii*50 : 46 + ii*50]], axis=0)\n",
    "        For_test_x = np.concatenate([For_test_x, n_frames[46 + ii*50 : 50 + ii*50]], axis=0) \n",
    "        For_test_y = np.concatenate([For_test_y, GEI_full_path[46 + ii*50 : 50 + ii*50]], axis=0)\n",
    "        \n",
    "with open('/usr/home/hao/work_space/FP/ITCNets/gait-data/OULP_GEI/GEI64x64_For_test_x', 'wb') as f:\n",
    "    np.save(f, For_test_x)\n",
    "with open('/usr/home/hao/work_space/FP/ITCNets/gait-data/OULP_GEI/GEI64x64_For_test_y', 'wb') as f:\n",
    "    np.save(f, For_test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batches(Subjects_path_X, Subjects_path_Y, batch_size):\n",
    "    \n",
    "    n_batches = len(Subjects_path_X)//batch_size\n",
    "    for ii in range(0, batch_size*n_batches, batch_size):\n",
    "        data_batch = []\n",
    "        target_batch = []\n",
    "        for each_path_X, each_path_Y in zip(Subjects_path_X[ii:ii + batch_size], Subjects_path_Y[ii:ii + batch_size]):\n",
    "            img_X = imread('{}'.format(each_path_X))\n",
    "            img_Y = imread('{}'.format(each_path_Y))\n",
    "            img_X = imresize(img_X, [64, 64], interp='nearest')\n",
    "            img_Y = imresize(img_Y, [64, 64], interp='nearest')\n",
    "            \n",
    "            data_batch.append(img_X)\n",
    "            target_batch.append(img_Y)\n",
    "        data_batch = np.array(data_batch)/255.0\n",
    "        target_batch = np.array(target_batch)/255.0\n",
    "        \n",
    "        data_batch = np.reshape(data_batch, [data_batch.shape[0], data_batch.shape[1], data_batch.shape[2], 1])\n",
    "        target_batch = np.reshape(target_batch, [target_batch.shape[0], target_batch.shape[1], target_batch.shape[2], 1])\n",
    "    \n",
    "        yield shuffle(data_batch, target_batch)\n",
    "\n",
    "def cal_accuracy(decoded, target):\n",
    "    error = abs(decoded - target)\n",
    "    Acc = (np.sum(error <= 0.08)/(decoded.shape[0]*decoded.shape[1]*decoded.shape[2]*decoded.shape[3]))*100\n",
    "    \n",
    "    return Acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_layer(inputs_, weight, weight_name, bias, bias_name, conv_name, pool_name, bn_name, relu_name, add_bacth_norm=True, is_training=True,\n",
    "               out_dim=1, conv_kernsize=(2,2), conv_strides=(1,1), pool_size=(2,2), pool_strides=(2,2), keep_prob=0.5):\n",
    "    ### Encoder\n",
    "    # e.g. out_dim: 8; kern_size: (3,3); pool_size: (2,2); strides: (2,2);\n",
    "    \n",
    "    # Weights\n",
    "    Weights = tf.Variable(weight, name=weight_name)\n",
    "    \n",
    "    # Bias\n",
    "    Bias = tf.Variable(bias, name=bias_name)\n",
    "    \n",
    "    # set strides of conv2d\n",
    "    stride = [1, conv_strides[0], conv_strides[1], 1]\n",
    "    \n",
    "    # conv2d\n",
    "    # filter:[filter_height, filter_width, in_channels, out_channels]\n",
    "    # input:[batch, in_height, in_width, in_channels]\n",
    "    conv2d = tf.nn.bias_add(tf.nn.conv2d(inputs_, Weights, stride, padding='SAME', name=conv_name), Bias)\n",
    "            \n",
    "    # add activation function\n",
    "    conv2d = tf.nn.relu(conv2d, name=relu_name)\n",
    "    \n",
    "    # add Max pooling\n",
    "    conv2d = tf.nn.max_pool(conv2d, [1,pool_size[0],pool_size[1],1],\\\n",
    "                                    [1,pool_strides[0],pool_strides[1],1], padding='SAME', name=pool_name)\n",
    "                    \n",
    "    conv2d = tf.layers.dropout(conv2d, rate=keep_prob)\n",
    "    \n",
    "    # Now 28x28x\n",
    "    if add_bacth_norm:\n",
    "        conv2d = tf.layers.batch_normalization(conv2d, training=is_training, name=bn_name)\n",
    "\n",
    "    return conv2d\n",
    "\n",
    "def deconv_layer(encoded, up_name, weight, weight_name, bias, bias_name, de_conv_name, bn_name, relu_name, add_batch_norm=True, is_training=True,\n",
    "                 up_size=[(2,2),(4,4),(5,5),(6,6)], out_dim=1, conv_kernsize=(2,2), conv_strides=(1,1), keep_prob=0.5):\n",
    "    ### Decoder\n",
    "    upsample = tf.image.resize_nearest_neighbor(encoded, up_size, name=up_name) # up_size: e.g.(7,7)\n",
    "    \n",
    "    # Weights\n",
    "    Weights = tf.Variable(weight, name=weight_name)\n",
    "    \n",
    "    # Bias\n",
    "    Bias = tf.Variable(bias, name=bias_name)\n",
    "    \n",
    "    # set strides of conv2d\n",
    "    stride = [1, conv_strides[0], conv_strides[1], 1]\n",
    "    \n",
    "    # conv2d\n",
    "    conv2d = tf.nn.bias_add(tf.nn.conv2d(upsample, Weights, stride, padding='SAME', name=de_conv_name),\\\n",
    "                   Bias)\n",
    "    \n",
    "    conv2d = tf.nn.relu(conv2d, name=relu_name)\n",
    "    \n",
    "    conv2d = tf.layers.dropout(conv2d, rate=keep_prob)\n",
    "    \n",
    "    if add_batch_norm:\n",
    "        conv2d = tf.layers.batch_normalization(conv2d, training=is_training, name=bn_name)\n",
    "\n",
    "    return conv2d\n",
    "\n",
    "def output_layer(conv2d, weight, weight_name, bias, bias_name, logits_name, bn_name, decoded_name, add_batch_norm=True, is_training=True, \n",
    "                 out_dim=1, conv_kernsize=(3,3), conv_strides=(1,1)):\n",
    "    # Weights\n",
    "    Weights = tf.Variable(weight, name=weight_name)\n",
    "    \n",
    "    # Bias\n",
    "    Bias = tf.Variable(bias, name=bias_name)\n",
    "    \n",
    "    # set strides of conv2d\n",
    "    stride = [1, conv_strides[0], conv_strides[1], 1]\n",
    "    \n",
    "    # conv2d\n",
    "    logits = tf.nn.bias_add(tf.nn.conv2d(conv2d, Weights, stride, padding='SAME', name=logits_name),\\\n",
    "                   Bias)\n",
    "    \n",
    "#     if add_batch_norm:\n",
    "#         conv2d = tf.layers.batch_normalization(conv2d, training=is_training, name=bn_name)\n",
    "    #Now 28x28x1\n",
    "    decoded = tf.nn.sigmoid(logits, name=decoded_name)\n",
    "    \n",
    "    return logits, decoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_kern_size = (4,4)\n",
    "conv_strides = (1,1)\n",
    "pool_size = (2,2)\n",
    "pool_strides = (2,2)\n",
    "up_size = [(16,16), (32,32), (64,64)]\n",
    "de_kern_size = (4,4)\n",
    "de_conv_strides = (1,1)\n",
    "out_kern_size = (4,4)\n",
    "out_strides = (1,1)\n",
    "\n",
    "\n",
    "g = tf.Graph()\n",
    "tf.reset_default_graph()\n",
    "with g.as_default():\n",
    "    \n",
    "    Au1_Weights_1 = tf.convert_to_tensor(Au1w1, dtype=tf.float32)\n",
    "    Au1_Bias_1 = tf.convert_to_tensor(Au1b1, dtype=tf.float32)\n",
    "    Au1_Weights_2 = tf.convert_to_tensor(Au1w2, dtype=tf.float32)\n",
    "    Au1_Bias_2 = tf.convert_to_tensor(Au1b2, dtype=tf.float32)\n",
    "    Au1_Weights_3 = tf.convert_to_tensor(Au1w3, dtype=tf.float32)\n",
    "    Au1_Bias_3 = tf.convert_to_tensor(Au1b3, dtype=tf.float32)\n",
    "    Au1_deWeights_1 = tf.convert_to_tensor(Au1deW1, dtype=tf.float32)\n",
    "    Au1_deBias_1 = tf.convert_to_tensor(Au1deB1, dtype=tf.float32)\n",
    "    \n",
    "    Au2_deWeights_1 = tf.convert_to_tensor(Au2deW1, dtype=tf.float32)\n",
    "    Au2_deBias_1 = tf.convert_to_tensor(Au2deB1, dtype=tf.float32)\n",
    "    \n",
    "    Au3_deWeights_1 = tf.convert_to_tensor(Au3deW1, dtype=tf.float32)\n",
    "    Au3_deBias_1 = tf.convert_to_tensor(Au3deB1, dtype=tf.float32)\n",
    "    \n",
    "    Au4_deWeights_1 = tf.convert_to_tensor(Au4deW1, dtype=tf.float32)\n",
    "    Au4_deBias_1 = tf.convert_to_tensor(Au4deB1, dtype=tf.float32)\n",
    "    \n",
    "    Au5_deWeights_1 = tf.convert_to_tensor(Au5deW1, dtype=tf.float32)\n",
    "    Au5_deBias_1 = tf.convert_to_tensor(Au5deB1, dtype=tf.float32)\n",
    "    \n",
    "    Au6_deWeights_1 = tf.convert_to_tensor(Au6deW1, dtype=tf.float32)\n",
    "    Au6_deBias_1 = tf.convert_to_tensor(Au6deB1, dtype=tf.float32)\n",
    "    \n",
    "    Au7_deWeights_1 = tf.convert_to_tensor(Au7deW1, dtype=tf.float32)\n",
    "    Au7_deBias_1 = tf.convert_to_tensor(Au7deB1, dtype=tf.float32)\n",
    "    \n",
    "    Au8_deWeights_1 = tf.convert_to_tensor(Au8deW1, dtype=tf.float32)\n",
    "    Au8_deBias_1 = tf.convert_to_tensor(Au8deB1, dtype=tf.float32)\n",
    "    \n",
    "    Au9_deWeights_1 = tf.convert_to_tensor(Au9deW1, dtype=tf.float32)\n",
    "    Au9_deBias_1 = tf.convert_to_tensor(Au9deB1, dtype=tf.float32)\n",
    "    Au9_deWeights_2 = tf.convert_to_tensor(Au9deW2, dtype=tf.float32)\n",
    "    Au9_deBias_2 = tf.convert_to_tensor(Au9deB2, dtype=tf.float32)\n",
    "    Au9_deWeights_3 = tf.convert_to_tensor(Au9deW3, dtype=tf.float32)\n",
    "    Au9_deBias_3 = tf.convert_to_tensor(Au9deB3, dtype=tf.float32)\n",
    "    Au9_outWeights = tf.convert_to_tensor(Au9outW, dtype=tf.float32)\n",
    "    Au9_outBias = tf.convert_to_tensor(Au9outB, dtype=tf.float32)\n",
    "    \n",
    "    learning_rate = tf.placeholder(tf.float32, name='learning_rate')\n",
    "    keep_p = tf.placeholder(tf.float32, name=\"keep_prob\")\n",
    "    training = tf.placeholder(tf.bool, name='training')\n",
    "    inputs = tf.placeholder(tf.float32, [None, 64, 64, 1], name='inputs')\n",
    "    targets = tf.placeholder(tf.float32, [None, 64, 64, 1], name='targets')\n",
    "\n",
    "    # 64x64\n",
    "    with tf.name_scope('conv_layer'):\n",
    "        # outdim 128\n",
    "        maxpool_1 = conv_layer(inputs, Au1_Weights_1, 'weight_1', Au1_Bias_1, 'bias_1', 'conv_1', 'pool_1', 'bn_1', 'relu_1', add_bacth_norm=False, \n",
    "                               is_training=training, out_dim=128, conv_kernsize=conv_kern_size, conv_strides=conv_strides, \n",
    "                               pool_size=pool_size, pool_strides=pool_strides, keep_prob=keep_p)\n",
    "        # 64\n",
    "        maxpool_2 = conv_layer(maxpool_1, Au1_Weights_2, 'weight_2', Au1_Bias_2, 'bias_2', 'conv_2', 'pool_2', 'bn_2', 'relu_2', add_bacth_norm=True, \n",
    "                               is_training=training, out_dim=64, conv_kernsize=conv_kern_size, conv_strides=conv_strides, \n",
    "                               pool_size=pool_size, pool_strides=pool_strides, keep_prob=keep_p)\n",
    "        # 32\n",
    "        maxpool_3 = conv_layer(maxpool_2, Au1_Weights_3, 'weight_3', Au1_Bias_3, 'bias_3', 'conv_3', 'pool_3', 'bn_3', 'relu_3', add_bacth_norm=True, \n",
    "                               is_training=training, out_dim=32, conv_kernsize=conv_kern_size, conv_strides=conv_strides, \n",
    "                               pool_size=pool_size, pool_strides=pool_strides, keep_prob=keep_p)\n",
    "        # 8x8x32\n",
    "    with tf.name_scope('hidden_layers'):\n",
    "        # outdim 8 -- 8\n",
    "        stride = [1, conv_strides[0], conv_strides[1], 1]\n",
    "        \n",
    "        # 1f - 3f \n",
    "        Au1deWeights1 = tf.Variable(Au1_deWeights_1, name='h_weight_1')\n",
    "        Au1deBias1 = tf.Variable(Au1_deBias_1, name='h_bias_1')\n",
    "        conv_hidden_1 = tf.nn.bias_add(tf.nn.conv2d(maxpool_3, Au1deWeights1, stride, padding='SAME', name='hidden_1'), Au1deBias1)\n",
    "        conv_hidden_1 = tf.layers.batch_normalization(conv_hidden_1, training=training, name='h_bn_1')\n",
    "        \n",
    "#         conv_hidden_1 = tf.nn.bias_add(tf.nn.conv2d(conv_hidden_1, Au1deWeights1, stride, padding='SAME', name='hidden_1_1'), Au1deBias1)\n",
    "#         conv_hidden_1 = tf.layers.batch_normalization(conv_hidden_1, training=training, name='h_bn_1_1')\n",
    "        \n",
    "#         conv_hidden_1 = tf.nn.bias_add(tf.nn.conv2d(conv_hidden_1, Au1deWeights1, stride, padding='SAME', name='hidden_1_2'), Au1deBias1)\n",
    "#         conv_hidden_1 = tf.layers.batch_normalization(conv_hidden_1, training=training, name='h_bn_1_2')\n",
    "        \n",
    "        # 3f - 5f \n",
    "        Au2deWeights1 = tf.Variable(Au2_deWeights_1, name='h_weight_2')\n",
    "        Au2deBias1 = tf.Variable(Au2_deBias_1, name='h_bias_2')\n",
    "        conv_hidden_2 = tf.nn.bias_add(tf.nn.conv2d(conv_hidden_1, Au2deWeights1, stride, padding='SAME', name='hidden_2'), Au2deBias1)\n",
    "        conv_hidden_2 = tf.layers.batch_normalization(conv_hidden_2, training=training, name='h_bn_2')\n",
    "        \n",
    "#         conv_hidden_2 = tf.nn.bias_add(tf.nn.conv2d(conv_hidden_2, Au2deWeights1, stride, padding='SAME', name='hidden_2_1'), Au2deBias1)\n",
    "#         conv_hidden_2 = tf.layers.batch_normalization(conv_hidden_2, training=training, name='h_bn_2_1')\n",
    "        \n",
    "#         conv_hidden_2 = tf.nn.bias_add(tf.nn.conv2d(conv_hidden_2, Au2deWeights1, stride, padding='SAME', name='hidden_2_2'), Au2deBias1)\n",
    "#         conv_hidden_2 = tf.layers.batch_normalization(conv_hidden_2, training=training, name='h_bn_2_2')\n",
    "\n",
    "        # 5f - 8f \n",
    "        Au3deWeights1 = tf.Variable(Au3_deWeights_1, name='h_weight_3')\n",
    "        Au3deBias1 = tf.Variable(Au3_deBias_1, name='h_bias_3')\n",
    "        conv_hidden_3 = tf.nn.bias_add(tf.nn.conv2d(conv_hidden_2, Au3deWeights1, stride, padding='SAME', name='hidden_3'), Au3deBias1)\n",
    "        conv_hidden_3 = tf.layers.batch_normalization(conv_hidden_3, training=training, name='h_bn_3')\n",
    "        \n",
    "#         conv_hidden_3 = tf.nn.bias_add(tf.nn.conv2d(conv_hidden_3, Au3deWeights1, stride, padding='SAME', name='hidden_3_1'), Au3deBias1)\n",
    "#         conv_hidden_3 = tf.layers.batch_normalization(conv_hidden_3, training=training, name='h_bn_3_1')\n",
    "    \n",
    "#         conv_hidden_3 = tf.nn.bias_add(tf.nn.conv2d(conv_hidden_3, Au3deWeights1, stride, padding='SAME', name='hidden_3_2'), Au3deBias1)\n",
    "#         conv_hidden_3 = tf.layers.batch_normalization(conv_hidden_3, training=training, name='h_bn_3_2')\n",
    "        # 8f - 10f \n",
    "        Au4deWeights1 = tf.Variable(Au4_deWeights_1, name='h_weight_4')\n",
    "        Au4deBias1 = tf.Variable(Au4_deBias_1, name='h_bias_4')\n",
    "        conv_hidden_4 = tf.nn.bias_add(tf.nn.conv2d(conv_hidden_3, Au4deWeights1, stride, padding='SAME', name='hidden_3'), Au4deBias1)\n",
    "        conv_hidden_4 = tf.layers.batch_normalization(conv_hidden_4, training=training, name='h_bn_4')\n",
    "        \n",
    "        # 10f - 13f \n",
    "        Au5deWeights1 = tf.Variable(Au5_deWeights_1, name='h_weight_5')\n",
    "        Au5deBias1 = tf.Variable(Au5_deBias_1, name='h_bias_5')\n",
    "        conv_hidden_5 = tf.nn.bias_add(tf.nn.conv2d(conv_hidden_4, Au5deWeights1, stride, padding='SAME', name='hidden_5'), Au5deBias1)\n",
    "        conv_hidden_5 = tf.layers.batch_normalization(conv_hidden_5, training=training, name='h_bn_5')\n",
    "        \n",
    "        # 13f - 15f\n",
    "        Au6deWeights1 = tf.Variable(Au6_deWeights_1, name='h_weight_6')\n",
    "        Au6deBias1 = tf.Variable(Au6_deBias_1, name='h_bias_6')\n",
    "        conv_hidden_6 = tf.nn.bias_add(tf.nn.conv2d(conv_hidden_5, Au6deWeights1, stride, padding='SAME', name='hidden_6'), Au6deBias1)\n",
    "        conv_hidden_6 = tf.layers.batch_normalization(conv_hidden_6, training=training, name='h_bn_6')\n",
    "        \n",
    "        # 15f - 18f\n",
    "        Au7deWeights1 = tf.Variable(Au7_deWeights_1, name='h_weight_7')\n",
    "        Au7deBias1 = tf.Variable(Au7_deBias_1, name='h_bias_7')\n",
    "        conv_hidden_7 = tf.nn.bias_add(tf.nn.conv2d(conv_hidden_6, Au7deWeights1, stride, padding='SAME', name='hidden_7'), Au7deBias1)\n",
    "        conv_hidden_7 = tf.layers.batch_normalization(conv_hidden_7, training=training, name='h_bn_7')\n",
    "        \n",
    "        # 18f - 20f\n",
    "        Au8deWeights1 = tf.Variable(Au8_deWeights_1, name='h_weight_8')\n",
    "        Au8deBias1 = tf.Variable(Au8_deBias_1, name='h_bias_8')\n",
    "        conv_hidden_8 = tf.nn.bias_add(tf.nn.conv2d(conv_hidden_7, Au8deWeights1, stride, padding='SAME', name='hidden_8'), Au8deBias1)\n",
    "        conv_hidden_8 = tf.layers.batch_normalization(conv_hidden_8, training=training, name='h_bn_8')\n",
    "        \n",
    "#         conv_hidden_4 = tf.nn.bias_add(tf.nn.conv2d(conv_hidden_4, Au4deWeights1, stride, padding='SAME', name='hidden_4_1'), Au4deBias1)\n",
    "#         conv_hidden_4 = tf.layers.batch_normalization(conv_hidden_4, training=training, name='h_bn_4_1')\n",
    "        \n",
    "#         conv_hidden_4 = tf.nn.bias_add(tf.nn.conv2d(conv_hidden_4, Au4deWeights1, stride, padding='SAME', name='hidden_4_2'), Au4deBias1)\n",
    "#         conv_hidden_4 = tf.layers.batch_normalization(conv_hidden_4, training=training, name='h_bn_4_2')\n",
    "\n",
    "    with tf.name_scope('deconv_layers'):\n",
    "        # outdim  -- 8\n",
    "        deconv_1 = deconv_layer(conv_hidden_8, 'up_1', Au9_deWeights_1, 'deWeight_1', Au9_deBias_1, 'deBias_1', 'deconv_1', 'debn_1', 'derelu_1', add_batch_norm=True, is_training=training,\n",
    "                     up_size=up_size[0], out_dim=32, conv_kernsize=de_kern_size, conv_strides=de_conv_strides, keep_prob=keep_p)\n",
    "        # 32\n",
    "        deconv_2 = deconv_layer(deconv_1, 'up_2', Au9_deWeights_2, 'deWeight_2', Au9_deBias_2, 'deBias_2', 'deconv_2', 'debn_2', 'derelu_2', add_batch_norm=True, is_training=training,\n",
    "                     up_size=up_size[1], out_dim=64, conv_kernsize=de_kern_size, conv_strides=de_conv_strides, keep_prob=keep_p)\n",
    "        # 64\n",
    "        deconv_3 = deconv_layer(deconv_2, 'up_3', Au9_deWeights_3, 'deWeight_3', Au9_deBias_3, 'deBias_3', 'deconv_3', 'debn_3', 'derelu_3', add_batch_norm=True, is_training=training,\n",
    "                     up_size=up_size[2], out_dim=128, conv_kernsize=de_kern_size, conv_strides=de_conv_strides, keep_prob=keep_p)\n",
    "        # 128\n",
    "    with tf.name_scope('output_layer'):\n",
    "        # outdim 128 -- 1\n",
    "        logits, decoded = output_layer(deconv_3, Au9_outWeights, 'outWeight', Au9_outBias, 'outBias', 'logits', 'outbn', 'output', add_batch_norm=True, is_training=training, \n",
    "                                       out_dim=1, conv_kernsize=out_kern_size, conv_strides=out_strides)\n",
    "\n",
    "    # Now 64x64x1 Output\n",
    "    loss = tf.nn.sigmoid_cross_entropy_with_logits(labels=targets, logits=logits, name='loss')\n",
    "    cost = tf.reduce_mean(loss, name='cost')\n",
    "    \n",
    "    MSE = tf.reduce_mean(tf.losses.mean_squared_error(targets, decoded), name='MSE')\n",
    "#     loss = tf.nn.sigmoid_cross_entropy_with_logits(labels=targets, logits=logits, name='loss')\n",
    "#     cost = tf.reduce_mean(loss, name='cost')\n",
    "    opt = tf.train.AdamOptimizer(learning_rate).minimize(MSE)\n",
    "\n",
    "    with tf.name_scope('saver'):\n",
    "        saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start training\n",
      "session start\n",
      "The 0-th epochs start:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/home/hao/miniconda3/envs/tensorflow/lib/python3.5/site-packages/ipykernel_launcher.py:10: DeprecationWarning: `imresize` is deprecated!\n",
      "`imresize` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
      "Use ``skimage.transform.resize`` instead.\n",
      "  # Remove the CWD from sys.path while we load stuff.\n",
      "/usr/home/hao/miniconda3/envs/tensorflow/lib/python3.5/site-packages/ipykernel_launcher.py:11: DeprecationWarning: `imresize` is deprecated!\n",
      "`imresize` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
      "Use ``skimage.transform.resize`` instead.\n",
      "  # This is added back by InteractiveShellApp.init_path()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/5000,  Iteration: 25,  Train mse: 0.0196,  2.8s /batch.  Train Accuracy: %72.7185\n",
      "\n",
      "Validation mse: 0.02004,  Validation accuracy: 73.161,\n",
      "\n",
      "Epoch: 1/5000,  Iteration: 50,  Train mse: 0.0143,  2.8s /batch.  Train Accuracy: %77.3987\n",
      "Epoch: 1/5000,  Iteration: 75,  Train mse: 0.0125,  2.8s /batch.  Train Accuracy: %79.0417\n",
      "The 1-th epochs start:\n",
      "Epoch: 2/5000,  Iteration: 100,  Train mse: 0.0113,  2.8s /batch.  Train Accuracy: %79.6301\n",
      "\n",
      "Validation mse: 0.01216,  Validation accuracy: 79.602,\n",
      "\n",
      "Epoch: 2/5000,  Iteration: 125,  Train mse: 0.0126,  3.0s /batch.  Train Accuracy: %78.9294\n",
      "Epoch: 2/5000,  Iteration: 150,  Train mse: 0.0113,  3.2s /batch.  Train Accuracy: %79.7583\n",
      "The 2-th epochs start:\n",
      "Epoch: 3/5000,  Iteration: 175,  Train mse: 0.0114,  3.3s /batch.  Train Accuracy: %79.3121\n",
      "Epoch: 3/5000,  Iteration: 200,  Train mse: 0.0113,  3.0s /batch.  Train Accuracy: %79.8770\n",
      "\n",
      "Validation mse: 0.01133,  Validation accuracy: 80.207,\n",
      "\n",
      "Epoch: 3/5000,  Iteration: 225,  Train mse: 0.0109,  2.9s /batch.  Train Accuracy: %80.5176\n",
      "The 3-th epochs start:\n",
      "Epoch: 4/5000,  Iteration: 250,  Train mse: 0.0115,  3.0s /batch.  Train Accuracy: %79.5139\n",
      "Epoch: 4/5000,  Iteration: 275,  Train mse: 0.0102,  3.2s /batch.  Train Accuracy: %80.5856\n",
      "Epoch: 4/5000,  Iteration: 300,  Train mse: 0.0101,  3.0s /batch.  Train Accuracy: %80.7193\n",
      "\n",
      "Validation mse: 0.01116,  Validation accuracy: 80.385,\n",
      "\n",
      "The 4-th epochs start:\n",
      "Epoch: 5/5000,  Iteration: 325,  Train mse: 0.0111,  2.9s /batch.  Train Accuracy: %79.5670\n",
      "Epoch: 5/5000,  Iteration: 350,  Train mse: 0.0109,  2.8s /batch.  Train Accuracy: %79.8950\n",
      "Epoch: 5/5000,  Iteration: 375,  Train mse: 0.0101,  2.9s /batch.  Train Accuracy: %80.4761\n",
      "Epoch: 5/5000,  Iteration: 400,  Train mse: 0.0094,  2.9s /batch.  Train Accuracy: %80.4941\n",
      "\n",
      "Validation mse: 0.01109,  Validation accuracy: 80.324,\n",
      "\n",
      "The 5-th epochs start:\n",
      "Epoch: 6/5000,  Iteration: 425,  Train mse: 0.0116,  2.9s /batch.  Train Accuracy: %80.3674\n",
      "Epoch: 6/5000,  Iteration: 450,  Train mse: 0.0103,  2.9s /batch.  Train Accuracy: %80.7874\n",
      "Epoch: 6/5000,  Iteration: 475,  Train mse: 0.0118,  3.1s /batch.  Train Accuracy: %79.7473\n",
      "The 6-th epochs start:\n",
      "Epoch: 7/5000,  Iteration: 500,  Train mse: 0.0110,  2.9s /batch.  Train Accuracy: %79.1409\n",
      "\n",
      "Validation mse: 0.01105,  Validation accuracy: 80.155,\n",
      "\n",
      "Epoch: 7/5000,  Iteration: 525,  Train mse: 0.0101,  2.9s /batch.  Train Accuracy: %80.3360\n",
      "Epoch: 7/5000,  Iteration: 550,  Train mse: 0.0108,  3.7s /batch.  Train Accuracy: %80.0836\n",
      "The 7-th epochs start:\n",
      "Epoch: 8/5000,  Iteration: 575,  Train mse: 0.0117,  3.6s /batch.  Train Accuracy: %79.7638\n",
      "Epoch: 8/5000,  Iteration: 600,  Train mse: 0.0099,  2.9s /batch.  Train Accuracy: %80.4099\n",
      "\n",
      "Validation mse: 0.01109,  Validation accuracy: 80.449,\n",
      "\n",
      "Epoch: 8/5000,  Iteration: 625,  Train mse: 0.0095,  3.6s /batch.  Train Accuracy: %80.9000\n",
      "The 8-th epochs start:\n",
      "Epoch: 9/5000,  Iteration: 650,  Train mse: 0.0102,  3.6s /batch.  Train Accuracy: %79.9139\n",
      "Epoch: 9/5000,  Iteration: 675,  Train mse: 0.0099,  3.6s /batch.  Train Accuracy: %80.0134\n",
      "Epoch: 9/5000,  Iteration: 700,  Train mse: 0.0108,  3.7s /batch.  Train Accuracy: %80.2640\n",
      "\n",
      "Validation mse: 0.01103,  Validation accuracy: 80.395,\n",
      "\n",
      "Epoch: 9/5000,  Iteration: 725,  Train mse: 0.0120,  3.6s /batch.  Train Accuracy: %79.5416\n",
      "The 9-th epochs start:\n",
      "Epoch: 10/5000,  Iteration: 750,  Train mse: 0.0110,  3.6s /batch.  Train Accuracy: %80.1892\n",
      "Epoch: 10/5000,  Iteration: 775,  Train mse: 0.0097,  3.6s /batch.  Train Accuracy: %80.6223\n",
      "Epoch: 10/5000,  Iteration: 800,  Train mse: 0.0102,  3.6s /batch.  Train Accuracy: %80.3833\n",
      "\n",
      "Validation mse: 0.01112,  Validation accuracy: 80.466,\n",
      "\n",
      "The 10-th epochs start:\n",
      "Epoch: 11/5000,  Iteration: 825,  Train mse: 0.0104,  3.6s /batch.  Train Accuracy: %80.3763\n",
      "Epoch: 11/5000,  Iteration: 850,  Train mse: 0.0103,  3.8s /batch.  Train Accuracy: %80.4608\n",
      "Epoch: 11/5000,  Iteration: 875,  Train mse: 0.0093,  3.5s /batch.  Train Accuracy: %80.6744\n",
      "The 11-th epochs start:\n",
      "Epoch: 12/5000,  Iteration: 900,  Train mse: 0.0110,  3.6s /batch.  Train Accuracy: %79.1254\n",
      "\n",
      "Validation mse: 0.01109,  Validation accuracy: 80.263,\n",
      "\n",
      "Epoch: 12/5000,  Iteration: 925,  Train mse: 0.0087,  4.2s /batch.  Train Accuracy: %80.9045\n",
      "Epoch: 12/5000,  Iteration: 950,  Train mse: 0.0107,  3.6s /batch.  Train Accuracy: %80.1227\n",
      "The 12-th epochs start:\n",
      "Epoch: 13/5000,  Iteration: 975,  Train mse: 0.0108,  3.6s /batch.  Train Accuracy: %79.7470\n",
      "Epoch: 13/5000,  Iteration: 1000,  Train mse: 0.0103,  3.6s /batch.  Train Accuracy: %80.2237\n",
      "\n",
      "Validation mse: 0.01116,  Validation accuracy: 80.391,\n",
      "\n",
      "Epoch: 13/5000,  Iteration: 1025,  Train mse: 0.0102,  3.6s /batch.  Train Accuracy: %79.8645\n",
      "Epoch: 13/5000,  Iteration: 1050,  Train mse: 0.0102,  2.8s /batch.  Train Accuracy: %79.8410\n",
      "The 13-th epochs start:\n",
      "Epoch: 14/5000,  Iteration: 1075,  Train mse: 0.0117,  2.8s /batch.  Train Accuracy: %79.2126\n",
      "Epoch: 14/5000,  Iteration: 1100,  Train mse: 0.0105,  2.8s /batch.  Train Accuracy: %80.3027\n",
      "\n",
      "Validation mse: 0.01106,  Validation accuracy: 80.257,\n",
      "\n",
      "Epoch: 14/5000,  Iteration: 1125,  Train mse: 0.0101,  2.8s /batch.  Train Accuracy: %80.2362\n",
      "The 14-th epochs start:\n",
      "Epoch: 15/5000,  Iteration: 1150,  Train mse: 0.0085,  2.8s /batch.  Train Accuracy: %81.9464\n",
      "Epoch: 15/5000,  Iteration: 1175,  Train mse: 0.0098,  2.8s /batch.  Train Accuracy: %80.4520\n",
      "Epoch: 15/5000,  Iteration: 1200,  Train mse: 0.0096,  2.8s /batch.  Train Accuracy: %80.5710\n",
      "\n",
      "Validation mse: 0.01116,  Validation accuracy: 80.275,\n",
      "\n",
      "The 15-th epochs start:\n",
      "Epoch: 16/5000,  Iteration: 1225,  Train mse: 0.0100,  2.8s /batch.  Train Accuracy: %80.5493\n",
      "Epoch: 16/5000,  Iteration: 1250,  Train mse: 0.0108,  2.8s /batch.  Train Accuracy: %79.4565\n",
      "Epoch: 16/5000,  Iteration: 1275,  Train mse: 0.0095,  2.8s /batch.  Train Accuracy: %80.5621\n",
      "The 16-th epochs start:\n",
      "Epoch: 17/5000,  Iteration: 1300,  Train mse: 0.0123,  2.8s /batch.  Train Accuracy: %79.2065\n",
      "\n",
      "Validation mse: 0.01116,  Validation accuracy: 80.214,\n",
      "\n",
      "Epoch: 17/5000,  Iteration: 1325,  Train mse: 0.0099,  2.8s /batch.  Train Accuracy: %81.0461\n",
      "Epoch: 17/5000,  Iteration: 1350,  Train mse: 0.0100,  2.8s /batch.  Train Accuracy: %80.7559\n",
      "Epoch: 17/5000,  Iteration: 1375,  Train mse: 0.0108,  2.8s /batch.  Train Accuracy: %79.8035\n",
      "The 17-th epochs start:\n",
      "Epoch: 18/5000,  Iteration: 1400,  Train mse: 0.0099,  2.8s /batch.  Train Accuracy: %80.4291\n",
      "\n",
      "Validation mse: 0.01129,  Validation accuracy: 80.130,\n",
      "\n",
      "Epoch: 18/5000,  Iteration: 1425,  Train mse: 0.0098,  2.8s /batch.  Train Accuracy: %80.9149\n",
      "Epoch: 18/5000,  Iteration: 1450,  Train mse: 0.0100,  2.8s /batch.  Train Accuracy: %80.8325\n",
      "The 18-th epochs start:\n",
      "Epoch: 19/5000,  Iteration: 1475,  Train mse: 0.0091,  2.8s /batch.  Train Accuracy: %80.9891\n",
      "Epoch: 19/5000,  Iteration: 1500,  Train mse: 0.0096,  2.8s /batch.  Train Accuracy: %81.2378\n",
      "\n",
      "Validation mse: 0.01139,  Validation accuracy: 80.418,\n",
      "\n",
      "Epoch: 19/5000,  Iteration: 1525,  Train mse: 0.0096,  2.8s /batch.  Train Accuracy: %80.9738\n",
      "The 19-th epochs start:\n",
      "Epoch: 20/5000,  Iteration: 1550,  Train mse: 0.0090,  2.8s /batch.  Train Accuracy: %81.1066\n",
      "Epoch: 20/5000,  Iteration: 1575,  Train mse: 0.0099,  2.8s /batch.  Train Accuracy: %80.9201\n",
      "Epoch: 20/5000,  Iteration: 1600,  Train mse: 0.0099,  2.8s /batch.  Train Accuracy: %80.3250\n",
      "\n",
      "Validation mse: 0.01137,  Validation accuracy: 80.345,\n",
      "\n",
      "The 20-th epochs start:\n",
      "Epoch: 21/5000,  Iteration: 1625,  Train mse: 0.0095,  2.8s /batch.  Train Accuracy: %80.5267\n",
      "Epoch: 21/5000,  Iteration: 1650,  Train mse: 0.0104,  2.8s /batch.  Train Accuracy: %80.2478\n",
      "Epoch: 21/5000,  Iteration: 1675,  Train mse: 0.0090,  2.8s /batch.  Train Accuracy: %81.2411\n",
      "Epoch: 21/5000,  Iteration: 1700,  Train mse: 0.0090,  2.8s /batch.  Train Accuracy: %81.0349\n",
      "\n",
      "Validation mse: 0.01169,  Validation accuracy: 80.187,\n",
      "\n",
      "The 21-th epochs start:\n",
      "Epoch: 22/5000,  Iteration: 1725,  Train mse: 0.0099,  2.8s /batch.  Train Accuracy: %80.8078\n",
      "Epoch: 22/5000,  Iteration: 1750,  Train mse: 0.0090,  2.8s /batch.  Train Accuracy: %80.2328\n",
      "Epoch: 22/5000,  Iteration: 1775,  Train mse: 0.0098,  2.8s /batch.  Train Accuracy: %80.4013\n",
      "The 22-th epochs start:\n",
      "Epoch: 23/5000,  Iteration: 1800,  Train mse: 0.0094,  2.8s /batch.  Train Accuracy: %80.8514\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation mse: 0.01221,  Validation accuracy: 80.287,\n",
      "\n",
      "Epoch: 23/5000,  Iteration: 1825,  Train mse: 0.0088,  2.8s /batch.  Train Accuracy: %81.5063\n",
      "Epoch: 23/5000,  Iteration: 1850,  Train mse: 0.0090,  2.8s /batch.  Train Accuracy: %80.9995\n",
      "The 23-th epochs start:\n",
      "Epoch: 24/5000,  Iteration: 1875,  Train mse: 0.0092,  2.8s /batch.  Train Accuracy: %80.7718\n",
      "Epoch: 24/5000,  Iteration: 1900,  Train mse: 0.0093,  2.8s /batch.  Train Accuracy: %81.2585\n",
      "\n",
      "Validation mse: 0.01216,  Validation accuracy: 80.162,\n",
      "\n",
      "Epoch: 24/5000,  Iteration: 1925,  Train mse: 0.0086,  2.8s /batch.  Train Accuracy: %81.4185\n",
      "The 24-th epochs start:\n",
      "Epoch: 25/5000,  Iteration: 1950,  Train mse: 0.0088,  2.8s /batch.  Train Accuracy: %81.4258\n",
      "Epoch: 25/5000,  Iteration: 1975,  Train mse: 0.0080,  2.8s /batch.  Train Accuracy: %81.3840\n",
      "Epoch: 25/5000,  Iteration: 2000,  Train mse: 0.0089,  2.8s /batch.  Train Accuracy: %81.3788\n",
      "\n",
      "Validation mse: 0.01231,  Validation accuracy: 80.027,\n",
      "\n",
      "Epoch: 25/5000,  Iteration: 2025,  Train mse: 0.0085,  2.8s /batch.  Train Accuracy: %81.8073\n",
      "The 25-th epochs start:\n",
      "Epoch: 26/5000,  Iteration: 2050,  Train mse: 0.0086,  2.8s /batch.  Train Accuracy: %81.4432\n",
      "Epoch: 26/5000,  Iteration: 2075,  Train mse: 0.0089,  2.8s /batch.  Train Accuracy: %81.0840\n",
      "Epoch: 26/5000,  Iteration: 2100,  Train mse: 0.0087,  2.8s /batch.  Train Accuracy: %81.4758\n",
      "\n",
      "Validation mse: 0.01224,  Validation accuracy: 79.982,\n",
      "\n",
      "The 26-th epochs start:\n",
      "Epoch: 27/5000,  Iteration: 2125,  Train mse: 0.0081,  2.8s /batch.  Train Accuracy: %82.2220\n",
      "Epoch: 27/5000,  Iteration: 2150,  Train mse: 0.0094,  2.8s /batch.  Train Accuracy: %81.0138\n",
      "Epoch: 27/5000,  Iteration: 2175,  Train mse: 0.0081,  2.8s /batch.  Train Accuracy: %82.0132\n",
      "The 27-th epochs start:\n",
      "Epoch: 28/5000,  Iteration: 2200,  Train mse: 0.0087,  2.8s /batch.  Train Accuracy: %81.1557\n",
      "\n",
      "Validation mse: 0.01279,  Validation accuracy: 79.805,\n",
      "\n",
      "Epoch: 28/5000,  Iteration: 2225,  Train mse: 0.0087,  2.8s /batch.  Train Accuracy: %81.6681\n",
      "Epoch: 28/5000,  Iteration: 2250,  Train mse: 0.0080,  2.8s /batch.  Train Accuracy: %81.7511\n",
      "The 28-th epochs start:\n",
      "Epoch: 29/5000,  Iteration: 2275,  Train mse: 0.0081,  2.8s /batch.  Train Accuracy: %82.0804\n",
      "Epoch: 29/5000,  Iteration: 2300,  Train mse: 0.0079,  2.8s /batch.  Train Accuracy: %82.3257\n",
      "\n",
      "Validation mse: 0.01289,  Validation accuracy: 80.009,\n",
      "\n",
      "Epoch: 29/5000,  Iteration: 2325,  Train mse: 0.0074,  2.8s /batch.  Train Accuracy: %82.8082\n",
      "The 29-th epochs start:\n",
      "Epoch: 30/5000,  Iteration: 2350,  Train mse: 0.0082,  2.8s /batch.  Train Accuracy: %81.5515\n",
      "Epoch: 30/5000,  Iteration: 2375,  Train mse: 0.0081,  2.8s /batch.  Train Accuracy: %81.5137\n",
      "Epoch: 30/5000,  Iteration: 2400,  Train mse: 0.0077,  2.8s /batch.  Train Accuracy: %82.5696\n",
      "\n",
      "Validation mse: 0.01280,  Validation accuracy: 80.036,\n",
      "\n",
      "Epoch: 30/5000,  Iteration: 2425,  Train mse: 0.0070,  2.8s /batch.  Train Accuracy: %82.6532\n",
      "The 30-th epochs start:\n",
      "Epoch: 31/5000,  Iteration: 2450,  Train mse: 0.0084,  2.8s /batch.  Train Accuracy: %82.3209\n",
      "Epoch: 31/5000,  Iteration: 2475,  Train mse: 0.0078,  2.8s /batch.  Train Accuracy: %82.7451\n",
      "Epoch: 31/5000,  Iteration: 2500,  Train mse: 0.0078,  2.8s /batch.  Train Accuracy: %82.0456\n",
      "\n",
      "Validation mse: 0.01301,  Validation accuracy: 79.849,\n",
      "\n",
      "The 31-th epochs start:\n",
      "Epoch: 32/5000,  Iteration: 2525,  Train mse: 0.0081,  2.8s /batch.  Train Accuracy: %81.5765\n",
      "Epoch: 32/5000,  Iteration: 2550,  Train mse: 0.0071,  2.8s /batch.  Train Accuracy: %82.6981\n",
      "Epoch: 32/5000,  Iteration: 2575,  Train mse: 0.0071,  2.8s /batch.  Train Accuracy: %82.6108\n",
      "The 32-th epochs start:\n",
      "Epoch: 33/5000,  Iteration: 2600,  Train mse: 0.0073,  2.8s /batch.  Train Accuracy: %82.8079\n",
      "\n",
      "Validation mse: 0.01336,  Validation accuracy: 79.894,\n",
      "\n",
      "Epoch: 33/5000,  Iteration: 2625,  Train mse: 0.0071,  2.8s /batch.  Train Accuracy: %82.7405\n",
      "Epoch: 33/5000,  Iteration: 2650,  Train mse: 0.0070,  2.8s /batch.  Train Accuracy: %83.4644\n",
      "The 33-th epochs start:\n",
      "Epoch: 34/5000,  Iteration: 2675,  Train mse: 0.0068,  2.8s /batch.  Train Accuracy: %83.1479\n",
      "Epoch: 34/5000,  Iteration: 2700,  Train mse: 0.0073,  2.8s /batch.  Train Accuracy: %82.2238\n",
      "\n",
      "Validation mse: 0.01372,  Validation accuracy: 79.674,\n",
      "\n",
      "Epoch: 34/5000,  Iteration: 2725,  Train mse: 0.0078,  2.8s /batch.  Train Accuracy: %82.6682\n",
      "Epoch: 34/5000,  Iteration: 2750,  Train mse: 0.0079,  2.8s /batch.  Train Accuracy: %82.3074\n",
      "The 34-th epochs start:\n",
      "Epoch: 35/5000,  Iteration: 2775,  Train mse: 0.0075,  2.8s /batch.  Train Accuracy: %82.7316\n",
      "Epoch: 35/5000,  Iteration: 2800,  Train mse: 0.0077,  2.8s /batch.  Train Accuracy: %82.8473\n",
      "\n",
      "Validation mse: 0.01324,  Validation accuracy: 79.707,\n",
      "\n",
      "Epoch: 35/5000,  Iteration: 2825,  Train mse: 0.0075,  2.8s /batch.  Train Accuracy: %82.6071\n",
      "The 35-th epochs start:\n",
      "Epoch: 36/5000,  Iteration: 2850,  Train mse: 0.0066,  2.8s /batch.  Train Accuracy: %83.4802\n",
      "Epoch: 36/5000,  Iteration: 2875,  Train mse: 0.0073,  2.8s /batch.  Train Accuracy: %83.4793\n",
      "Epoch: 36/5000,  Iteration: 2900,  Train mse: 0.0071,  2.8s /batch.  Train Accuracy: %82.7576\n",
      "\n",
      "Validation mse: 0.01364,  Validation accuracy: 79.756,\n",
      "\n",
      "The 36-th epochs start:\n",
      "Epoch: 37/5000,  Iteration: 2925,  Train mse: 0.0073,  2.8s /batch.  Train Accuracy: %82.3239\n",
      "Epoch: 37/5000,  Iteration: 2950,  Train mse: 0.0066,  2.8s /batch.  Train Accuracy: %83.6172\n",
      "Epoch: 37/5000,  Iteration: 2975,  Train mse: 0.0073,  2.8s /batch.  Train Accuracy: %82.5983\n",
      "The 37-th epochs start:\n",
      "Epoch: 38/5000,  Iteration: 3000,  Train mse: 0.0070,  2.8s /batch.  Train Accuracy: %83.0234\n",
      "\n",
      "Validation mse: 0.01390,  Validation accuracy: 79.449,\n",
      "\n",
      "Epoch: 38/5000,  Iteration: 3025,  Train mse: 0.0070,  2.8s /batch.  Train Accuracy: %82.9990\n",
      "Epoch: 38/5000,  Iteration: 3050,  Train mse: 0.0067,  2.8s /batch.  Train Accuracy: %83.0444\n",
      "Epoch: 38/5000,  Iteration: 3075,  Train mse: 0.0072,  2.8s /batch.  Train Accuracy: %82.5800\n",
      "The 38-th epochs start:\n",
      "Epoch: 39/5000,  Iteration: 3100,  Train mse: 0.0070,  2.8s /batch.  Train Accuracy: %82.9428\n",
      "\n",
      "Validation mse: 0.01375,  Validation accuracy: 79.828,\n",
      "\n",
      "Epoch: 39/5000,  Iteration: 3125,  Train mse: 0.0070,  2.8s /batch.  Train Accuracy: %83.1516\n",
      "Epoch: 39/5000,  Iteration: 3150,  Train mse: 0.0066,  2.8s /batch.  Train Accuracy: %83.2520\n",
      "The 39-th epochs start:\n",
      "Epoch: 40/5000,  Iteration: 3175,  Train mse: 0.0059,  2.8s /batch.  Train Accuracy: %84.5584\n",
      "Epoch: 40/5000,  Iteration: 3200,  Train mse: 0.0065,  2.8s /batch.  Train Accuracy: %83.4818\n",
      "\n",
      "Validation mse: 0.01354,  Validation accuracy: 80.069,\n",
      "\n",
      "Epoch: 40/5000,  Iteration: 3225,  Train mse: 0.0062,  2.8s /batch.  Train Accuracy: %83.2391\n",
      "The 40-th epochs start:\n",
      "Epoch: 41/5000,  Iteration: 3250,  Train mse: 0.0063,  2.8s /batch.  Train Accuracy: %83.7122\n",
      "Epoch: 41/5000,  Iteration: 3275,  Train mse: 0.0069,  2.9s /batch.  Train Accuracy: %82.7655\n",
      "Epoch: 41/5000,  Iteration: 3300,  Train mse: 0.0065,  2.8s /batch.  Train Accuracy: %83.7451\n",
      "\n",
      "Validation mse: 0.01371,  Validation accuracy: 80.047,\n",
      "\n",
      "The 41-th epochs start:\n",
      "Epoch: 42/5000,  Iteration: 3325,  Train mse: 0.0071,  2.8s /batch.  Train Accuracy: %82.7545\n",
      "Epoch: 42/5000,  Iteration: 3350,  Train mse: 0.0063,  2.8s /batch.  Train Accuracy: %84.1010\n",
      "Epoch: 42/5000,  Iteration: 3375,  Train mse: 0.0063,  2.8s /batch.  Train Accuracy: %84.1754\n",
      "Epoch: 42/5000,  Iteration: 3400,  Train mse: 0.0070,  2.8s /batch.  Train Accuracy: %83.5022\n",
      "\n",
      "Validation mse: 0.01381,  Validation accuracy: 80.224,\n",
      "\n",
      "The 42-th epochs start:\n",
      "Epoch: 43/5000,  Iteration: 3425,  Train mse: 0.0060,  2.8s /batch.  Train Accuracy: %84.1513\n",
      "Epoch: 43/5000,  Iteration: 3450,  Train mse: 0.0059,  2.8s /batch.  Train Accuracy: %84.2514\n",
      "Epoch: 43/5000,  Iteration: 3475,  Train mse: 0.0061,  2.8s /batch.  Train Accuracy: %83.8071\n",
      "The 43-th epochs start:\n",
      "Epoch: 44/5000,  Iteration: 3500,  Train mse: 0.0060,  2.8s /batch.  Train Accuracy: %84.1681\n",
      "\n",
      "Validation mse: 0.01431,  Validation accuracy: 79.726,\n",
      "\n",
      "Epoch: 44/5000,  Iteration: 3525,  Train mse: 0.0061,  2.8s /batch.  Train Accuracy: %84.3033\n",
      "Epoch: 44/5000,  Iteration: 3550,  Train mse: 0.0063,  2.8s /batch.  Train Accuracy: %83.9215\n",
      "The 44-th epochs start:\n",
      "Epoch: 45/5000,  Iteration: 3575,  Train mse: 0.0064,  2.8s /batch.  Train Accuracy: %83.5574\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 45/5000,  Iteration: 3600,  Train mse: 0.0064,  2.8s /batch.  Train Accuracy: %84.2059\n",
      "\n",
      "Validation mse: 0.01404,  Validation accuracy: 79.733,\n",
      "\n",
      "Epoch: 45/5000,  Iteration: 3625,  Train mse: 0.0060,  2.8s /batch.  Train Accuracy: %83.8062\n",
      "The 45-th epochs start:\n",
      "Epoch: 46/5000,  Iteration: 3650,  Train mse: 0.0060,  2.8s /batch.  Train Accuracy: %83.8638\n",
      "Epoch: 46/5000,  Iteration: 3675,  Train mse: 0.0063,  4.1s /batch.  Train Accuracy: %83.3835\n",
      "Epoch: 46/5000,  Iteration: 3700,  Train mse: 0.0061,  2.8s /batch.  Train Accuracy: %83.9291\n",
      "\n",
      "Validation mse: 0.01401,  Validation accuracy: 79.927,\n",
      "\n",
      "Epoch: 46/5000,  Iteration: 3725,  Train mse: 0.0063,  2.8s /batch.  Train Accuracy: %84.2963\n",
      "The 46-th epochs start:\n",
      "Epoch: 47/5000,  Iteration: 3750,  Train mse: 0.0059,  2.8s /batch.  Train Accuracy: %84.2529\n",
      "Epoch: 47/5000,  Iteration: 3775,  Train mse: 0.0058,  2.8s /batch.  Train Accuracy: %84.2389\n",
      "Epoch: 47/5000,  Iteration: 3800,  Train mse: 0.0060,  2.8s /batch.  Train Accuracy: %83.8455\n",
      "\n",
      "Validation mse: 0.01438,  Validation accuracy: 79.827,\n",
      "\n",
      "The 47-th epochs start:\n",
      "Epoch: 48/5000,  Iteration: 3825,  Train mse: 0.0061,  2.8s /batch.  Train Accuracy: %83.9941\n",
      "Epoch: 48/5000,  Iteration: 3850,  Train mse: 0.0054,  2.8s /batch.  Train Accuracy: %84.9289\n",
      "Epoch: 48/5000,  Iteration: 3875,  Train mse: 0.0060,  2.8s /batch.  Train Accuracy: %84.4403\n",
      "The 48-th epochs start:\n",
      "Epoch: 49/5000,  Iteration: 3900,  Train mse: 0.0060,  2.8s /batch.  Train Accuracy: %83.6371\n",
      "\n",
      "Validation mse: 0.01402,  Validation accuracy: 79.716,\n",
      "\n",
      "Epoch: 49/5000,  Iteration: 3925,  Train mse: 0.0059,  2.8s /batch.  Train Accuracy: %84.2874\n",
      "Epoch: 49/5000,  Iteration: 3950,  Train mse: 0.0059,  2.8s /batch.  Train Accuracy: %84.5108\n",
      "The 49-th epochs start:\n",
      "Epoch: 50/5000,  Iteration: 3975,  Train mse: 0.0059,  2.8s /batch.  Train Accuracy: %84.1556\n",
      "Epoch: 50/5000,  Iteration: 4000,  Train mse: 0.0054,  2.8s /batch.  Train Accuracy: %84.6951\n",
      "\n",
      "Validation mse: 0.01434,  Validation accuracy: 79.768,\n",
      "\n",
      "Epoch: 50/5000,  Iteration: 4025,  Train mse: 0.0059,  2.8s /batch.  Train Accuracy: %84.1623\n",
      "Epoch: 50/5000,  Iteration: 4050,  Train mse: 0.0055,  2.8s /batch.  Train Accuracy: %85.4495\n",
      "The 50-th epochs start:\n",
      "Epoch: 51/5000,  Iteration: 4075,  Train mse: 0.0058,  2.8s /batch.  Train Accuracy: %84.8199\n",
      "Epoch: 51/5000,  Iteration: 4100,  Train mse: 0.0058,  3.4s /batch.  Train Accuracy: %84.1806\n",
      "\n",
      "Validation mse: 0.01419,  Validation accuracy: 79.649,\n",
      "\n",
      "Epoch: 51/5000,  Iteration: 4125,  Train mse: 0.0058,  3.7s /batch.  Train Accuracy: %84.6494\n",
      "The 51-th epochs start:\n",
      "Epoch: 52/5000,  Iteration: 4150,  Train mse: 0.0055,  4.4s /batch.  Train Accuracy: %84.9130\n",
      "Epoch: 52/5000,  Iteration: 4175,  Train mse: 0.0059,  3.7s /batch.  Train Accuracy: %84.5129\n",
      "Epoch: 52/5000,  Iteration: 4200,  Train mse: 0.0053,  3.6s /batch.  Train Accuracy: %84.9921\n",
      "\n",
      "Validation mse: 0.01404,  Validation accuracy: 79.726,\n",
      "\n",
      "The 52-th epochs start:\n",
      "Epoch: 53/5000,  Iteration: 4225,  Train mse: 0.0060,  4.0s /batch.  Train Accuracy: %83.8040\n",
      "Epoch: 53/5000,  Iteration: 4250,  Train mse: 0.0055,  3.7s /batch.  Train Accuracy: %85.0491\n",
      "Epoch: 53/5000,  Iteration: 4275,  Train mse: 0.0054,  4.6s /batch.  Train Accuracy: %85.2536\n",
      "The 53-th epochs start:\n",
      "Epoch: 54/5000,  Iteration: 4300,  Train mse: 0.0057,  4.4s /batch.  Train Accuracy: %84.7192\n",
      "\n",
      "Validation mse: 0.01474,  Validation accuracy: 79.389,\n",
      "\n",
      "Epoch: 54/5000,  Iteration: 4325,  Train mse: 0.0053,  3.7s /batch.  Train Accuracy: %85.0232\n",
      "Epoch: 54/5000,  Iteration: 4350,  Train mse: 0.0053,  4.8s /batch.  Train Accuracy: %85.6146\n",
      "The 54-th epochs start:\n",
      "Epoch: 55/5000,  Iteration: 4375,  Train mse: 0.0056,  3.7s /batch.  Train Accuracy: %84.5404\n",
      "Epoch: 55/5000,  Iteration: 4400,  Train mse: 0.0057,  3.6s /batch.  Train Accuracy: %84.4931\n",
      "\n",
      "Validation mse: 0.01452,  Validation accuracy: 79.602,\n",
      "\n",
      "Epoch: 55/5000,  Iteration: 4425,  Train mse: 0.0054,  3.6s /batch.  Train Accuracy: %85.0616\n",
      "Epoch: 55/5000,  Iteration: 4450,  Train mse: 0.0053,  3.5s /batch.  Train Accuracy: %85.5640\n",
      "The 55-th epochs start:\n",
      "Epoch: 56/5000,  Iteration: 4475,  Train mse: 0.0058,  3.6s /batch.  Train Accuracy: %84.8663\n",
      "Epoch: 56/5000,  Iteration: 4500,  Train mse: 0.0055,  3.7s /batch.  Train Accuracy: %85.5807\n",
      "\n",
      "Validation mse: 0.01477,  Validation accuracy: 79.469,\n",
      "\n",
      "Epoch: 56/5000,  Iteration: 4525,  Train mse: 0.0053,  3.7s /batch.  Train Accuracy: %84.6756\n",
      "The 56-th epochs start:\n",
      "Epoch: 57/5000,  Iteration: 4550,  Train mse: 0.0057,  3.7s /batch.  Train Accuracy: %84.5413\n"
     ]
    }
   ],
   "source": [
    "epochs = 5000\n",
    "num_layers = 3\n",
    "batch_size = 80\n",
    "val_batch_size = 50\n",
    "is_save = False\n",
    "dims = 128\n",
    "\n",
    "for lr in [0.00006]:\n",
    "    save_string = './checkpoints_view_invariant/All_full_Casia_6types_E2E_lr={}_bs={}_nl={}_dim={}.ckpt'.format(lr, batch_size,\n",
    "                                                                                                      num_layers, dims)\n",
    "    print(\"Start training\")\n",
    "    with tf.Session(config=tf.ConfigProto(log_device_placement=True), graph=g) as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "\n",
    "        iteration = 1\n",
    "#         mean_val_loss = 0\n",
    "        mean_val_acc = 0\n",
    "        mean_val_mse = 0\n",
    "#         count_loss_not_decrease_epochs = 0\n",
    "        count_acc_not_decrease_epochs = 0\n",
    "#         Last_val_loss = 0\n",
    "        Last_val_acc = 0\n",
    "\n",
    "        last_saved_acc = 0\n",
    "        print(\"session start\")\n",
    "        for e in range(epochs):\n",
    "            print(\"The {}-th epochs start:\".format(e))\n",
    "            for train_batch, target_batch in get_batches(train_x, train_y, batch_size):\n",
    "            \n",
    "                start = time.time()\n",
    "\n",
    "                feed_1 = {\n",
    "                            inputs: train_batch, \n",
    "                            targets: target_batch,\n",
    "                            keep_p: 0.5,\n",
    "                            learning_rate: lr,\n",
    "                            training:True\n",
    "                        }\n",
    "\n",
    "                train_mse, _, decoded_img = sess.run([MSE, opt, decoded], feed_dict=feed_1)\n",
    "\n",
    "                train_acc = cal_accuracy(decoded_img, target_batch)\n",
    "\n",
    "                if iteration%25==0:\n",
    "                    end = time.time()\n",
    "                    print(\"Epoch: {}/{},\".format(e+1, epochs),' '\n",
    "                              \"Iteration: {},\".format(iteration),' '\n",
    "                              \"Train mse: {:.4f},\".format(train_mse),' '\n",
    "                              \"{:.1f}s /batch.\".format((end-start)/5),' '\n",
    "                              \"Train Accuracy: %{:.4f}\".format(train_acc))\n",
    "\n",
    "                    ##############################################################\n",
    "                    ######################## VALIDATION ##########################\n",
    "                    ##############################################################\n",
    "\n",
    "                if iteration == 25 or iteration%100==0:\n",
    "                    validation_loss = []\n",
    "                    validation_acc = []\n",
    "                    validation_mse = []\n",
    "\n",
    "\n",
    "                    for ii, (val_batch, val_target_batch) in enumerate(get_batches(val_x, val_y, val_batch_size)):\n",
    "                        feed_2 = {\n",
    "                                    inputs: val_batch,\n",
    "                                    targets: val_target_batch,\n",
    "                                    keep_p: 1,\n",
    "                                    training:True\n",
    "                                }\n",
    "\n",
    "                        val_mse, val_decoded_img = sess.run([MSE, decoded], feed_dict=feed_2)\n",
    "\n",
    "                        val_acc = cal_accuracy(val_decoded_img, val_target_batch)\n",
    "    \n",
    "#                         validation_loss.append(val_loss)\n",
    "                        validation_acc.append(val_acc)\n",
    "                        validation_mse.append(val_mse)\n",
    "\n",
    "#                     Last_val_loss = mean_val_loss\n",
    "                    \n",
    "#                     mean_val_loss = np.mean(np.array(validation_loss))\n",
    "                    mean_val_acc = np.mean(np.array(validation_acc))\n",
    "                    mean_val_mse = np.mean(np.array(validation_mse))\n",
    "\n",
    "                    print()\n",
    "                    print(\n",
    "                          \"Validation mse: {:.5f},\".format(mean_val_mse),' '\n",
    "                          \"Validation accuracy: {:.3f},\".format(mean_val_acc))\n",
    "                    print()\n",
    "                    if (mean_val_acc > 92.1) & (mean_val_acc > last_saved_acc):\n",
    "                        saver.save(sess, r\"{}\".format(save_string)) \n",
    "                        last_saved_acc = mean_val_acc\n",
    "                        print('###################################model been saved mean accuracy={:.4f}, measn MSE={:.4f}'.format(mean_val_acc, mean_val_mse))\n",
    "                    \n",
    "\n",
    "                iteration += 1\n",
    "#         saver.save(sess, r\"{}\".format(save_string))  \n",
    "print(' ')\n",
    "print(' ')\n",
    "print(\"leraning_rate={},num_layers={},batch_size={} finished, saved\".format(lr, num_layers, batch_size))\n",
    "print(' ')\n",
    "print(' ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br><br><br><br><br><br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "val_x = np.load(open(r'../gait_data/OULP_GEI/GEI64x64_val_x', mode='rb'))\n",
    "val_y = np.load(open(r'../gait_data/OULP_GEI/GEI64x64_val_y', mode='rb'))\n",
    "test_x = np.load(open(r'../gait_data/OULP_GEI/GEI64x64_test_x', mode='rb'))\n",
    "test_y = np.load(open(r'../gait_data/OULP_GEI/GEI64x64_test_y', mode='rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_batches(Subjects_path_X, Subjects_path_Y, batch_size):\n",
    "    \n",
    "    n_batches = len(Subjects_path_X)//batch_size\n",
    "    for ii in range(0, batch_size*n_batches, batch_size):\n",
    "        data_batch = []\n",
    "        target_batch = []\n",
    "        for each_path_X, each_path_Y in zip(Subjects_path_X[ii:ii + batch_size], Subjects_path_Y[ii:ii + batch_size]):\n",
    "            img_X = imread('{}'.format(each_path_X))\n",
    "            img_Y = imread('{}'.format(each_path_Y))\n",
    "            img_X = imresize(img_X, [64, 64], interp='nearest')\n",
    "            img_Y = imresize(img_Y, [64, 64], interp='nearest')\n",
    "            \n",
    "            data_batch.append(img_X)\n",
    "            target_batch.append(img_Y)\n",
    "        data_batch = np.array(data_batch)/float(255)\n",
    "        target_batch = np.array(target_batch)/float(255)\n",
    "        \n",
    "        data_batch = np.reshape(data_batch, [data_batch.shape[0], data_batch.shape[1], data_batch.shape[2], 1])\n",
    "        target_batch = np.reshape(target_batch, [target_batch.shape[0], target_batch.shape[1], target_batch.shape[2], 1])\n",
    "    \n",
    "        yield data_batch, target_batch\n",
    "        \n",
    "def cal_accuracy(decoded, target):\n",
    "    error = abs(decoded - target)\n",
    "    Acc = (np.sum(error <= 0.08)/(decoded.shape[0]*decoded.shape[1]*decoded.shape[2]*decoded.shape[3]))*100\n",
    "    \n",
    "    return Acc\n",
    "\n",
    "def save_decoded(decoded_batch, save_path, batch_number, test_batch_size):\n",
    "    reshape = np.reshape(decoded_batch, [decoded_batch.shape[0], decoded_batch.shape[1], decoded_batch.shape[2]])\n",
    "    for ii, image in enumerate(reshape):\n",
    "        imsave('{}/{}.png'.format(save_path, ii + batch_number*test_batch_size + 1), image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save Target: used to generate predicted complete GEIs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-25-d5f66fa5b2f8>, line 16)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-25-d5f66fa5b2f8>\"\u001b[0;36m, line \u001b[0;32m16\u001b[0m\n\u001b[0;31m    test_y[ii*500 + 7000 + idx*14000:(ii+1)*500 + 7000 + idx*14000]], axis=0)\u001b[0m\n\u001b[0m         ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for idx, n_frames in enumerate(['1f', '3f', '5f', '8f', '10f', '13f', '15f', '18f', '20f']): \n",
    "    for g_or_p in ['gallery', 'probe']:\n",
    "\n",
    "        for ii in range(0,14):\n",
    "\n",
    "            if g_or_p == 'gallery':\n",
    "                t_x = np.concatenate([val_x[ii*500 + idx*14000:(ii+1)*500 + idx*14000], \n",
    "                                          test_x[ii*500 + idx*14000:(ii+1)*500 + idx*14000]], axis=0) \n",
    "                t_y = np.concatenate([val_y[ii*500 + idx*14000:(ii+1)*500 + idx*14000], \n",
    "                                          test_y[ii*500 + idx*14000:(ii+1)*500 + idx*14000]], axis=0)    \n",
    "            else:\n",
    "                t_x = np.concatenate([val_x[ii*500 + 7000 + idx*14000:(ii+1)*500 + 7000 + idx*14000], \n",
    "                                          test_x[ii*500 + 7000 + idx*14000:(ii+1)*500 + 7000 + idx*14000]], axis=0) \n",
    "                t_y = np.concatenate([val_y[ii*500 + 7000 + idx*14000:(ii+1)*500 + 7000 + idx*14000], val_x\n",
    "                                          test_y[ii*500 + 7000 + idx*14000:(ii+1)*500 + 7000 + idx*14000]], axis=0)  \n",
    "\n",
    "            New_folder_path = '/usr/home/hao/work_space/FP/ITCNets/OULP-C1V2_Pack/OULP-GEI-(64x64)-{}-complete_End-to-End/decoded_{}_{}-complete_{}'.format(n_frames, g_or_p, n_frames, ii+1)\n",
    "            os.makedirs(New_folder_path)\n",
    "\n",
    "            test_batch_size = 20\n",
    "            loaded_graph = tf.Graph()\n",
    "            save_model_path = './checkpoints_view_invariant/choosed/All-2_full_add_more_data_E2E_lr=6e-05_bs=80_nl=3_dim=128.ckpt'\n",
    "            with tf.Session(graph=loaded_graph) as sess:\n",
    "                    # Load model\n",
    "                loader = tf.train.import_meta_graph(save_model_path + '.meta')\n",
    "                loader.restore(sess, save_model_path)\n",
    "\n",
    "                    # Get Tensors from loaded model\n",
    "    #             test_inputs = loaded_graph.get_tensor_by_name('inputs_targets/inputs_1:0')\n",
    "    #             test_targets = loaded_graph.get_tensor_by_name('inputs_targets/targets_1:0')\n",
    "    #             test_training = loaded_graph.get_tensor_by_name('training:0')\n",
    "    #             test_keep_p = loaded_graph.get_tensor_by_name('keep_prob:0')\n",
    "\n",
    "    #             test_decoded = loaded_graph.get_tensor_by_name('output_layer/decoded:0')\n",
    "    #             test_MSE= loaded_graph.get_tensor_by_name('MSE:0')\n",
    "                test_inputs = loaded_graph.get_tensor_by_name('inputs:0')\n",
    "                test_targets = loaded_graph.get_tensor_by_name('targets:0')\n",
    "                test_training = loaded_graph.get_tensor_by_name('training:0')\n",
    "                test_keep_p = loaded_graph.get_tensor_by_name('keep_prob:0')\n",
    "                test_decoded = loaded_graph.get_tensor_by_name('output_layer/output:0')\n",
    "                test_MSE = loaded_graph.get_tensor_by_name('MSE:0')\n",
    "\n",
    "                test_mse = []\n",
    "                test_acc = []\n",
    "                mean_test_mse = 0\n",
    "                mean_test_acc = 0\n",
    "\n",
    "                for idx2, (test_batch, test_target_batch) in enumerate(get_batches(t_x, t_y, test_batch_size)):\n",
    "            #                                          np.concatenate([val_x[3000+ii*500:3000+(ii+1)*500], test_x[3000+ii*500:3000+(ii+1)*500]], axis=0), \n",
    "            #                                          np.concatenate([val_y[3000+ii*500:3000+(ii+1)*500], test_y[3000+ii*500:3000+(ii+1)*500]], axis=0), test_batch_size)):\n",
    "\n",
    "\n",
    "                    feed_dict = {test_inputs: test_batch,\n",
    "                                    test_targets: test_target_batch,\n",
    "                                    test_training: 1,\n",
    "                                    test_training:True\n",
    "                                    }\n",
    "\n",
    "                    test_mean_squared_error, test_decoded_img = sess.run([test_MSE, test_decoded], feed_dict=feed_dict)\n",
    "                    save_decoded(test_decoded_img, New_folder_path, idx2, test_batch_size)\n",
    "\n",
    "                    test_ACC = cal_accuracy(test_decoded_img, test_target_batch)\n",
    "\n",
    "                    test_mse.append(test_mean_squared_error)\n",
    "                    test_acc.append(test_ACC)\n",
    "\n",
    "                mean_test_mse = np.mean(np.array(test_mse))\n",
    "                mean_test_acc = np.mean(np.array(test_acc))\n",
    "\n",
    "                print('mean_test_mse: {:.4f}'.format(mean_test_mse))\n",
    "                print('mean_test_acc: {:.4f}'.format(mean_test_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# calculate mean loss and mean acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for n_frames in ['1f', '3f', '5f', '8f', '10f', '13f', '15f', '18f', '20f']:\n",
    "    n_frames_list = np.array([])\n",
    "    \n",
    "    for g_or_p in ['gallery', 'probe']:\n",
    "        for start_frames in np.arange(1, 15):\n",
    "            \n",
    "            path = r'./OULP-C1V2_Pack/OULP-GEI-(64x64)-{}-complete_End-to-End/decoded_{}_{}-complete_{}'.format(n_frames, g_or_p, n_frames, start_frames)\n",
    "            path_list = []\n",
    "            for ii in np.arange(1, 1001):\n",
    "                path_list.append(os.path.join(path, '{}.png'.format(ii)))\n",
    "            n_frames_list = np.concatenate([n_frames_list, np.array(path_list)], axis=0)\n",
    "        \n",
    "    with open('../gait_data/OULP_GEI/PredforTest_{}'.format(n_frames), 'wb') as f:\n",
    "        np.save(f, n_frames_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "PredforTest_1f = np.load(open(r'../gait_data/OULP_GEI/PredforTest_1f', mode='rb'))\n",
    "PredforTest_3f = np.load(open(r'../gait_data/OULP_GEI/PredforTest_3f', mode='rb'))\n",
    "PredforTest_5f = np.load(open(r'../gait_data/OULP_GEI/PredforTest_5f', mode='rb'))\n",
    "PredforTest_8f = np.load(open(r'../gait_data/OULP_GEI/PredforTest_8f', mode='rb'))\n",
    "PredforTest_10f = np.load(open(r'../gait_data/OULP_GEI/PredforTest_10f', mode='rb'))\n",
    "PredforTest_13f = np.load(open(r'../gait_data/OULP_GEI/PredforTest_13f', mode='rb'))\n",
    "PredforTest_15f = np.load(open(r'../gait_data/OULP_GEI/PredforTest_15f', mode='rb'))\n",
    "PredforTest_18f = np.load(open(r'../gait_data/OULP_GEI/PredforTest_18f', mode='rb'))\n",
    "PredforTest_20f = np.load(open(r'../gait_data/OULP_GEI/PredforTest_20f', mode='rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gallery_y = np.concatenate([val_y[:500], test_y[:500]], axis=0)\n",
    "\n",
    "prob_y = np.concatenate([val_y[7000:7500], test_y[7000:7500]], axis=0)\n",
    "\n",
    "target_y = np.array([])\n",
    "for i in range(14):\n",
    "    target_y = np.concatenate([target_y, gallery_y])\n",
    "for i in range(14):\n",
    "    target_y = np.concatenate([target_y, prob_y])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from skimage.measure import compare_ssim as ssim\n",
    "def mse(imageA, imageB):\n",
    "\t# the 'Mean Squared Error' between the two images is the\n",
    "\t# sum of the squared difference between the two images;\n",
    "\t# NOTE: the two images must have the same dimension\n",
    "\terr = np.sum((imageA.astype(\"float\") - imageB.astype(\"float\")) ** 2)\n",
    "\terr /= float(imageA.shape[0] * imageA.shape[1])\n",
    "\t\n",
    "\t# return the MSE, the lower the error, the more \"similar\"\n",
    "\t# the two images are\n",
    "\treturn err\n",
    "\n",
    "def cal_mean_accuracy(decoded, target):\n",
    "    error = abs(decoded - target)\n",
    "    Acc = (np.sum(error <= 0.08)/(decoded.shape[0]*decoded.shape[1]))*100\n",
    "    \n",
    "    return Acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Std_m = []\n",
    "Std_s = []\n",
    "Std_acc = []\n",
    "M = []\n",
    "S = []\n",
    "ACC = []\n",
    "test_set = PredforTest_8f\n",
    "\n",
    "for g_or_p in ['gallery', 'probe']:\n",
    "    for idx in range(14):\n",
    "        if g_or_p == 'gallery':\n",
    "            pred_idx = test_set[1000*idx:1000 + 1000*idx]\n",
    "            target_idx = target_y[1000*idx:1000 + 1000*idx]\n",
    "        else:\n",
    "            pred_idx = test_set[14000+1000*idx:14000 + 1000 + 1000*idx]\n",
    "            target_idx = target_y[14000+1000*idx:14000 + 1000 + 1000*idx]\n",
    "            \n",
    "        m  = []\n",
    "        s = []\n",
    "        acc = []\n",
    "        for ii in range(1000):\n",
    "            pred = imread(pred_idx[ii])\n",
    "            target = imresize(imread(target_idx[ii]), [64,64], interp='nearest')/255.0\n",
    "            \n",
    "            m.append(mse(pred.astype(float), target.astype(float)))\n",
    "            s.append(ssim(pred.astype(float), target.astype(float)))\n",
    "            acc.append(cal_mean_accuracy(pred.astype(float), target.astype(float)))\n",
    "        \n",
    "        Std_m.append(m)\n",
    "        Std_s.append(s)\n",
    "        Std_acc.append(acc)\n",
    "        M.append(np.mean(np.array(m)))\n",
    "        S.append(np.mean(np.array(s)))\n",
    "        ACC.append(np.mean(np.array(acc)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean mse 0.0011\n",
      "mse std 0.0005\n",
      "             \n",
      "mean ssim 0.9608\n",
      "std ssim 0.0172\n",
      "             \n",
      "mean Acc 95.7515\n",
      "std Acc 1.7986\n"
     ]
    }
   ],
   "source": [
    "M = np.array(M)\n",
    "S = np.array(S)\n",
    "ACC = np.array(ACC)\n",
    "\n",
    "MSE = (M[:14] + M[14:])/2\n",
    "SSIM = (S[:14] + S[14:])/2\n",
    "ACCURACY = (ACC[:14] + ACC[14:])/2\n",
    "\n",
    "print('mean mse {:.4f}'.format(np.mean(MSE)))\n",
    "print('mse std {:.4f}'.format(np.std(np.array(Std_m))))\n",
    "print('             ')\n",
    "\n",
    "print('mean ssim {:.4f}'.format(np.mean(SSIM)))\n",
    "print('std ssim {:.4f}'.format(np.std(np.array(Std_s))))\n",
    "print('             ')\n",
    "\n",
    "print('mean Acc {:.4f}'.format(np.mean(ACCURACY)))\n",
    "print('std Acc {:.4f}'.format(np.std(np.array(Std_acc))))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# For 8f and 5f: special analysis for 8f- and 5f-GEIs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.00196034  0.00182863  0.00173614  0.00166996  0.00162427  0.00159619\n",
      "  0.00157686  0.00156963  0.00156994  0.00158062  0.00160662  0.00161695\n",
      "  0.00162767  0.00162767]\n",
      "[ 0.93926193  0.94220752  0.944054    0.94539066  0.94614033  0.94652871\n",
      "  0.94699966  0.94711546  0.94701964  0.94681884  0.94632665  0.94617521\n",
      "  0.94603191  0.94603189]\n",
      "[ 92.98574219  93.32380371  93.54870605  93.72481689  93.84476318\n",
      "  93.9090332   93.95615234  93.96318359  93.95638428  93.9138916\n",
      "  93.83094482  93.79033203  93.7713623   93.7713623 ]\n"
     ]
    }
   ],
   "source": [
    "# print(MSE)\n",
    "# print(SSIM)\n",
    "# print(ACCURACY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mse_std 1 0.000858310706927\n",
      "mse_std 2 0.000809244851539\n",
      "mse_std 3 0.000780463641407\n",
      "mse_std 4 0.00075977314634\n",
      "mse_std 5 0.000736109389893\n",
      "mse_std 6 0.000723497995406\n",
      "mse_std 7 0.000720240582511\n",
      "mse_std 8 0.000710607135813\n",
      "mse_std 9 0.000696723892108\n",
      "mse_std 10 0.000690347257702\n",
      "mse_std 11 0.000713174311563\n",
      "mse_std 12 0.00072189751038\n",
      "mse_std 13 0.000724640677855\n",
      "mse_std 14 0.000724641329305\n",
      "ssim_std 1 0.0249089323319\n",
      "ssim_std 2 0.023940389953\n",
      "ssim_std 3 0.0234903125471\n",
      "ssim_std 4 0.0233414657102\n",
      "ssim_std 5 0.0229198411906\n",
      "ssim_std 6 0.0230153153243\n",
      "ssim_std 7 0.0231563116756\n",
      "ssim_std 8 0.0233388422884\n",
      "ssim_std 9 0.0233505296555\n",
      "ssim_std 10 0.0230882481978\n",
      "ssim_std 11 0.0235714505414\n",
      "ssim_std 12 0.0236524835299\n",
      "ssim_std 13 0.023442301545\n",
      "ssim_std 14 0.0234423446165\n",
      "acc_std 1 2.52974771542\n",
      "acc_std 2 2.4361914706\n",
      "acc_std 3 2.41754415047\n",
      "acc_std 4 2.39652689376\n",
      "acc_std 5 2.36521472595\n",
      "acc_std 6 2.36961622932\n",
      "acc_std 7 2.38374658805\n",
      "acc_std 8 2.35370102477\n",
      "acc_std 9 2.37475986744\n",
      "acc_std 10 2.418704712\n",
      "acc_std 11 2.46116212816\n",
      "acc_std 12 2.46862970083\n",
      "acc_std 13 2.44081076417\n",
      "acc_std 14 2.44078646613\n"
     ]
    }
   ],
   "source": [
    "Std_m = np.array(Std_m)\n",
    "Std_s = np.array(Std_s)\n",
    "Std_acc = np.array(Std_acc)\n",
    "\n",
    "# STD_m = (Std_m[:14] + Std_m[14:])/2\n",
    "# STD_s = (Std_s[:14] + Std_s[14:])/2\n",
    "# STD_acc = (Std_acc[:14] + Std_acc[14:])/2\n",
    "\n",
    "for i in range(0, 14):\n",
    "    print('mse_std {}'.format(i+1), np.std(np.concatenate([Std_m[i], Std_m[i+14]], axis=0)))\n",
    "    \n",
    "for i in range(0, 14):\n",
    "    print('ssim_std {}'.format(i+1), np.std(np.concatenate([Std_s[i], Std_s[i+14]], axis=0)))\n",
    "for i in range(0, 14):\n",
    "    print('acc_std {}'.format(i+1), np.std(np.concatenate([Std_acc[i], Std_acc[i+14]], axis=0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 5f-full\n",
    "[ 0.00196034  0.00182863  0.00173614  0.00166996  0.00162427  0.00159619\n",
    "  0.00157686  0.00156963  0.00156994  0.00158062  0.00160662  0.00161695\n",
    "  0.00162767  0.00162767]\n",
    "[ 0.93926193  0.94220752  0.944054    0.94539066  0.94614033  0.94652871\n",
    "  0.94699966  0.94711546  0.94701964  0.94681884  0.94632665  0.94617521\n",
    "  0.94603191  0.94603189]\n",
    "[ 92.98574219  93.32380371  93.54870605  93.72481689  93.84476318\n",
    "  93.9090332   93.95615234  93.96318359  93.95638428  93.9138916\n",
    "  93.83094482  93.79033203  93.7713623   93.7713623 ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 8f-full\n",
    "[ 0.00196034  0.00182863  0.00173614  0.00166996  0.00162427  0.00159619\n",
    "  0.00157686  0.00156963  0.00156994  0.00158062  0.00160662  0.00161695\n",
    "  0.00162767  0.00162767]\n",
    "[ 0.93926193  0.94220752  0.944054    0.94539066  0.94614033  0.94652871\n",
    "  0.94699966  0.94711546  0.94701964  0.94681884  0.94632665  0.94617521\n",
    "  0.94603191  0.94603189]\n",
    "[ 92.98574219  93.32380371  93.54870605  93.72481689  93.84476318\n",
    "  93.9090332   93.95615234  93.96318359  93.95638428  93.9138916\n",
    "  93.83094482  93.79033203  93.7713623   93.7713623 ]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# get some samples: to generate some samples used in my thesis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'plt' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-5aa6d38c6559>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mPredforTest_1f\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'plt' is not defined"
     ]
    }
   ],
   "source": [
    "plt.imshow(imread(PredforTest_1f[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f02b2115828>"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAMEAAAD8CAYAAADOpsDvAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nCZTAo0qgBB5VAiXwqBIoged/AayDdEECbw8EAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f02b197ecf8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(imread(val_x[3500]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'val_x' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-758d57b9b33b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'1f-Complete'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0midx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mval_x\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mtarget\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtarget_y\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'val_x' is not defined"
     ]
    }
   ],
   "source": [
    "name = '1f-Complete'\n",
    "idx = 0\n",
    "inputs = val_x\n",
    "target = target_y\n",
    "\n",
    "in_imgs = []\n",
    "target_imgs = []\n",
    "save_in_imgs = np.ndarray([64, 11*64], dtype=float)\n",
    "save_target_imgs = np.ndarray([64, 11*64], dtype=float)\n",
    "\n",
    "for ii, each in enumerate(inputs[3500 + 14000*idx:3511 + 14000*idx]):\n",
    "    in_imgs.append(imresize(imread(each), [64,64], interp='nearest')/255.0)\n",
    "    save_in_imgs[:, ii*64:ii*64 + 64] = imresize(imread(each), [64,64], interp='nearest')/255.0\n",
    "    \n",
    "for ii,each in enumerate(target[7000:7011]):\n",
    "    target_imgs.append(imresize(imread(each), [64,64], interp='nearest')/255.0)\n",
    "    save_target_imgs[:, ii*64:ii*64 + 64] = imresize(imread(each), [64,64], interp='nearest')/255.0\n",
    "    \n",
    "in_imgs = np.array(in_imgs)\n",
    "target_imgs = np.array(target_imgs)\n",
    "\n",
    "New_folder_path = './OULP-C1V2_Pack/Samples/End-to-End/{}'.format(name)\n",
    "# os.makedirs(New_folder_path)\n",
    "\n",
    "# plt.imshow(in_imgs[7])\n",
    "# plt.savefig('./OULP-C1V2_Pack/Samples/End-to-End/{}/{}.eps'.format(name, 'in_single'), format='eps', dpi=1000)\n",
    "\n",
    "# plt.imshow(target_imgs[7])\n",
    "# plt.savefig('./OULP-C1V2_Pack/Samples/End-to-End/{}/{}.eps'.format(name, 'target_single'), format='eps', dpi=1000)\n",
    "# plt.imshow(save_in_imgs)\n",
    "imsave('./OULP-C1V2_Pack/Samples/End-to-End/{}/{}.png'.format(name, 'in_imgs'), save_in_imgs)\n",
    "# plt.savefig('./OULP-C1V2_Pack/Samples/End-to-End/{}/{}.eps'.format(name, 'in_imgs'), format='eps', dpi=1000)\n",
    "# plt.imshow(save_target_imgs)\n",
    "# plt.savefig('./OULP-C1V2_Pack/Samples/End-to-End/{}/{}.eps'.format(name, 'target_imgs'), format='eps', dpi=1000)\n",
    "imsave('./OULP-C1V2_Pack/Samples/End-to-End/{}/{}.png'.format(name, 'target_imgs'), save_target_imgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tf' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-7af4484c12e3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mloaded_graph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGraph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0msave_model_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'./checkpoints_view_invariant/choosed/All-2_full_add_more_data_E2E_lr=6e-05_bs=80_nl=3_dim=128.ckpt'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0min_imgs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0min_imgs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m11\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m64\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m64\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mtarget_imgs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget_imgs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m11\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m64\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m64\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'tf' is not defined"
     ]
    }
   ],
   "source": [
    "loaded_graph = tf.Graph()\n",
    "save_model_path = './checkpoints_view_invariant/choosed/All-2_full_add_more_data_E2E_lr=6e-05_bs=80_nl=3_dim=128.ckpt'\n",
    "in_imgs = np.reshape(in_imgs, [11, 64, 64, 1])\n",
    "target_imgs = np.reshape(target_imgs, [11, 64, 64, 1])\n",
    "\n",
    "with tf.Session(graph=loaded_graph) as sess:\n",
    "        # Load model\n",
    "    loader = tf.train.import_meta_graph(save_model_path + '.meta')\n",
    "    loader.restore(sess, save_model_path)\n",
    "\n",
    "        # Get Tensors from loaded model\n",
    "    test_inputs = loaded_graph.get_tensor_by_name('inputs:0')\n",
    "    test_targets = loaded_graph.get_tensor_by_name('targets:0')\n",
    "    test_training = loaded_graph.get_tensor_by_name('training:0')\n",
    "    test_keep_p = loaded_graph.get_tensor_by_name('keep_prob:0')\n",
    "    test_decoded = loaded_graph.get_tensor_by_name('output_layer/output:0')\n",
    "    test_MSE = loaded_graph.get_tensor_by_name('MSE:0')\n",
    "\n",
    "    \n",
    "    feed_dict = {test_inputs: in_imgs,\n",
    "                         test_training: 1,\n",
    "                         test_training:True\n",
    "                        }\n",
    "    \n",
    "    test_decoded_img = sess.run(test_decoded, feed_dict=feed_dict)\n",
    "    \n",
    "    test_decoded_img = np.reshape(test_decoded_img, [11, 64,64])\n",
    "    \n",
    "    save_pred_imgs = np.ndarray([64, 11*64], dtype=float)\n",
    "    \n",
    "    for ii,each in enumerate(test_decoded_img):\n",
    "        save_pred_imgs[:, ii*64:ii*64 + 64] = test_decoded_img[ii]\n",
    "        \n",
    "#     plt.imshow(test_decoded_img[7])\n",
    "#     plt.savefig('./OULP-C1V2_Pack/Samples/End-to-End/{}/{}.eps'.format(name, 'pred_single'), format='eps', dpi=1000)\n",
    "#     plt.imshow(save_pred_imgs)\n",
    "#     plt.savefig('./OULP-C1V2_Pack/Samples/End-to-End/{}/{}.eps'.format(name, 'pred_imgs'), format='eps', dpi=1000)\n",
    "    imsave('./OULP-C1V2_Pack/Samples/End-to-End/{}/{}.png'.format(name, 'pred_imgs'), save_pred_imgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'plt' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-7868299aa187>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'./OULP-C1V2_Pack/Samples/End-to-End/1f-Complete/target_imgs.eps'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'plt' is not defined"
     ]
    }
   ],
   "source": [
    "plt.imshow(imread('./OULP-C1V2_Pack/Samples/End-to-End/1f-Complete/target_imgs.eps'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
